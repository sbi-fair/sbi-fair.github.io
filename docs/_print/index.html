<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=/docs/><link rel=alternate type=application/rss+xml href=/docs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Documentation | SBI-FAIR</title><meta name=description content="A list of documents managed through the Web Site related to this project."><meta property="og:title" content="Documentation"><meta property="og:description" content="Powerful, extensible, and feature-packed frontend toolkit. Build and customize with Sass, utilize prebuilt grid system and components, and bring projects to life with powerful JavaScript plugins."><meta property="og:type" content="website"><meta property="og:url" content="/docs/"><meta itemprop=name content="Documentation"><meta itemprop=description content="Powerful, extensible, and feature-packed frontend toolkit. Build and customize with Sass, utilize prebuilt grid system and components, and bring projects to life with powerful JavaScript plugins."><meta name=twitter:card content="summary"><meta name=twitter:title content="Documentation"><meta name=twitter:description content="Powerful, extensible, and feature-packed frontend toolkit. Build and customize with Sass, utilize prebuilt grid system and components, and bring projects to life with powerful JavaScript plugins."><link rel=preload href=/scss/main.min.92d2bf82a410cd58c2b4fa1c71e96d2d6f4b42d746be6406a0dc6e77326dce57.css as=style integrity="sha256-ktK/gqQQzVjCtPocceltLW9LQtdGvmQGoNxudzJtzlc=" crossorigin=anonymous><link href=/scss/main.min.92d2bf82a410cd58c2b4fa1c71e96d2d6f4b42d746be6406a0dc6e77326dce57.css rel=stylesheet integrity="sha256-ktK/gqQQzVjCtPocceltLW9LQtdGvmQGoNxudzJtzlc=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class=navbar-brand__name>SBI-FAIR</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/publications/><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/docs/surrogates/><span>Surrogates</span></a></li><li class=nav-item><a class=nav-link href=/docs/software/><span>Software</span></a></li><li class=nav-item><a class=nav-link href=/docs/notes/><span>Notes</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/community/><span>Community</span></a></li></ul></div><div class="d-none d-lg-block"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Documentation</h1><ul><li>1: <a href=#pg-a692b14857d19c068ceed81ccb6b71d4>Abstract</a></li><ul></ul><li>2: <a href=#pg-e74dc031c69016f227f3aa7d249a0b2b>Introduction</a></li><ul></ul><li>3: <a href=#pg-89caff5907aed978dfb4a280efff213c>Metadata Subgroup</a></li><li>4: <a href=#pg-b764d436041462d3a04ca39c7db93384>Publications</a></li><ul></ul><li>5: <a href=#pg-4b7b381f29076b85f3dfb64e89a089c1>Team</a></li><li>6: <a href=#pg-bbcb72d0609a64a0b2c2b5b4b06fc741>Surrogates</a></li><ul><li>6.1: <a href=#pg-541c846fdc6fb55135b27065f2879d73>AutoPhaseNN: unsupervised physics-aware deep learning of 3D nanoscale Bragg coherent diffraction imaging</a></li><li>6.2: <a href=#pg-e87c590decacea82502a53d7c30c5035>Calorimeter surrogates</a></li><li>6.3: <a href=#pg-386ca1030e8123ddf937ecb1eac2905c>Virtual tissue</a></li><li>6.4: <a href=#pg-c81d71fde3c0ee4014394d74adc53de0>Cosmoflow</a></li><li>6.5: <a href=#pg-f33b00fce1cb69e54b44c3dacba3bfad>Fully ionized plasma fluid model closures</a></li><li>6.6: <a href=#pg-78e52a1336c6e0869c51f7cf3eef7d30>Ions in nanoconfinement</a></li><li>6.7: <a href=#pg-da679368f9d15b0486abed0d11a2ce9a>miniWeatherML</a></li><li>6.8: <a href=#pg-6e66623ebf80ce302fd399a8dd6f8d7b>OSMI</a></li><li>6.9: <a href=#pg-e825d671002d82761de7d86a532a7008>Particle dynamics</a></li><li>6.10: <a href=#pg-d65c9f63f13fcf64fe06b834161e6a0d>PtychoNN: deep learning network for ptychographic imaging that predicts sample amplitude and phase from diffraction data.</a></li></ul><li>7: <a href=#pg-dfd48793659645274fbc4a4e0fa01cc1>Software</a></li><ul><li>7.1: <a href=#pg-8f4c333ddb68886d5b962f16d12c4b30>cloudmesh</a></li><li>7.2: <a href=#pg-fb12f1a23bf3d5978f884fefe1d63f2b>sabath</a></li></ul><li>8: <a href=#pg-99bc04328a000750d727f1aa33ff141a>Meeting Notes</a></li><ul><li>8.1: <a href=#pg-8213bcaea95358ab160234a01af4dac8>Poster</a></li><li>8.2: <a href=#pg-71c2009b324938d14a8ce55b6942149b>Links</a></li><li>8.3: <a href=#pg-e65be94dc6e2f46c51835ce41a181d32>Meeting Notes 02-05-2024</a></li><li>8.4: <a href=#pg-7d149b5d4a85949257539522324849b2>Meeting Notes 01-08-2024</a></li><li>8.5: <a href=#pg-152bff6e97f1acbf5589970a9aa5a7d5>Meeting Notes 10-30-2023</a></li><li>8.6: <a href=#pg-e6023118fbee3e0ec2353782edd96ca9>Meeting Notes 09-25-2023</a></li><li>8.7: <a href=#pg-48d76162e8cb20ff3e36438d8bc97914>Meeting Notes 08-25-2023</a></li><li>8.8: <a href=#pg-1824fe440fafb28768087ce57fe99e41>Meeting Notes 07-31-2023</a></li><li>8.9: <a href=#pg-728a054cd992007e0a960e3857134b26>Meeting Notes 06-26-2023</a></li><li>8.10: <a href=#pg-0c727b9cc18c28c4dcea8e387a96729c>Meeting Notes 05-29-2023</a></li><li>8.11: <a href=#pg-1193689e8ba9961b6135bc169af74a93>Meeting Notes 04-03-2023</a></li><li>8.12: <a href=#pg-edb1615b81c946ffc8c736bb64715a13>Meeting Notes 02-27-2023</a></li><li>8.13: <a href=#pg-9702267f6758080ea10e254acb342cdf>Meeting Notes 01-30-2023</a></li><li>8.14: <a href=#pg-ad039a91808ad7bbe0578a4521b0524f>Meeting Notes 01-05-2023</a></li><li>8.15: <a href=#pg-6926e251b710e799f3ba9380666cb126>Meeting Notes 11-28-2022</a></li><li>8.16: <a href=#pg-9e87d0c6e4d002550de608a5cf492c85>Meeting Notes 10-31-2022</a></li><li>8.17: <a href=#pg-3829bc23ea44f58891cd7a2ddea19ffb>Meeting Notes 09-26-2022</a></li><li>8.18: <a href=#pg-1cd978086b98f4fae66b30ea114be812>Meeting Notes 08-15-2022</a></li><li>8.19: <a href=#pg-f3c338aaa79ded581231c48cbf59f301>Meeting Notes 06-27-2022</a></li><li>8.20: <a href=#pg-632e11d8ab45f2682589a545cce0fb97>Meeting Notes 05-23-2022</a></li><li>8.21: <a href=#pg-d5cc2405b43f81f520ab2bfe99534d50>Meeting Notes 04-25-2022</a></li><li>8.22: <a href=#pg-1ad0df7b5a7a4ca4d394fa6fff6de9da>Meeting Notes 03-19-2022</a></li><li>8.23: <a href=#pg-852e8f9f130ef25f4f6ff038135fb60d>Meeting Notes 02-14-2022</a></li><li>8.24: <a href=#pg-4d4a72ac924bdb915d2e0829bdd4b38d>Meeting Notes 01-10-2022</a></li><li>8.25: <a href=#pg-ef2e55e0086e76a1ad6d95adc4dc9c05>Meeting Notes 10-21-2021</a></li><li>8.26: <a href=#pg-7d0723c0cf7daa1c408381ee87d3ab00>Meeting Notes 09-27-2021</a></li><li>8.27: <a href=#pg-61514199af825bada68cfa586f122586>Meeting Notes 08-30-2021</a></li><li>8.28: <a href=#pg-a2f2a49a9c251a60b8b3d8d394c02960>Meeting Notes 07-26-2021</a></li><li>8.29: <a href=#pg-424ae80d730642b73c7dc54af84bdebd>Meeting Notes 06-29-2021</a></li><li>8.30: <a href=#pg-cd876d9c980694d77f84f5268583d645>Meeting Notes 05-24-2021</a></li><li>8.31: <a href=#pg-593c8019dfc9c8121a0bba92a829b7d2>Meeting Notes 04-19-2021</a></li><li>8.32: <a href=#pg-f6b9a39a5bbf5d8e177dea442a2437a6>Meeting Notes 03-23-2021</a></li><li>8.33: <a href=#pg-b80aeca90aa52cd52e412f58032ccd34>Meeting Notes 02-20-2021</a></li><li>8.34: <a href=#pg-2770df73b2e02cb315a180647d6fd023>Meeting Notes 01-20-2021</a></li></ul><li>9: <a href=#pg-68f9113b50693620ee9892f038d4139b>Contribution Guidelines</a></li><ul></ul></ul><div class=content><div class="pageinfo pageinfo-primary"><p>A list of documents managed through the Web Site related to this project.</p></div></div></div><div class=td-content><h1 id=pg-a692b14857d19c068ceed81ccb6b71d4>1 - Abstract</h1><div class=lead>A brief abstract about the project</div><div class="pageinfo pageinfo-primary"><p>The Surrogate Benchmark Initiative (SBI) abstract as presented at the DOE ASCR Meeting, Feb 2024</p></div><p>Replacing traditional HPC computations with deep learning surrogates can dramatically improve the performance of simulations. We need to build repositories for AI models, datasets, and results that are easily used with FAIR metadata. These must cover a broad spectrum of use cases and system issues. The need for heterogeneous architectures means new software and performance issues. Further surrogate performance models are needed. The SBI (Surrogate Benchmark Initiative) collaboration between Argonne National Lab, Indiana University, Rutgers, University of Tennessee, and Virginia (lead) with MLCommons addresses these issues. The collaboration accumulates existing and generates new surrogates and hosts them (a total of around 20) in repositories. Selected surrogates become MLCommons benchmarks. The surrogates are managed by a FAIR metadata system, SABATH, developed by Tennessee and implemented for our repositories by Virginia.
The surrogate domains are Bragg coherent diffraction imaging, ptychographic imaging, Fully ionized plasma fluid model closures, molecular dynamics(2),<br>turbulence in computational fluid dynamics, cosmology, Kaggle calorimeter challenge(4), virtual tissue simulations(2), and performance tuning. Rutgers built a taxonomy using previous work and protein-ligand docking, which will be quantified using six mini-apps representing the system structure for different surrogate uses. Argonne has studied the data-loading and I/O structure for deep learning using inter-epoch and intra-batch reordering to improve data reuse. Their system addresses communication with the aggregation of small messages. They also study second-order optimizers using compression balancing accuracy and compression level. Virginia has used I/O parallelization to further improve performance. Indiana looked at ways of reducing the needed training set size for a given surrogate accuracy.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>,<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>,<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><h2 id=refernces>Refernces<a class=td-heading-self-link href=#refernces aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Web Page for Surrogate Benchmark Initiative SBI: FAIR Surrogate Benchmarks Supporting AI and Simulation Research. Web Page, January 2024. URL: <a href=https://sbi-fair.github.io/>https://sbi-fair.github.io/</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Publications: <a href=https://sbi-fair.github.io/docs/publications/>https://sbi-fair.github.io/docs/publications/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Meeting summaries: <a href=https://docs.google.com/document/d/1cqMOkV9Cag6EB6HI6fR20gwhVwUeG5yijtJ3aEW0Crs/>https://docs.google.com/document/d/1cqMOkV9Cag6EB6HI6fR20gwhVwUeG5yijtJ3aEW0Crs/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e74dc031c69016f227f3aa7d249a0b2b>2 - Introduction</h1><div class=lead>A brief introduction to the project</div><div class="pageinfo pageinfo-primary"><p>The Surrogate Benchmark Initiative (SBI) project will create a community
repository and FAIR data ecosystem for HPC application surrogate benchmarks,
including data, code, and all relevant collateral artifacts the science and
engineering community needs to use and reuse these data sets and surrogates.</p></div><p>Like nearly every field of science and engineering today,
Computational Science using High Performance Computing (HPC) is being
transformed by the ongoing revolution in Artificial Intelligence (AI),
especially by the use of data-driven Deep Neural Network (DNN)
techniques. In particular, DNN surrogate models <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, are
being used to replace either part or all of traditional large-scale
HPC simulations, achieving remarkable performance improvements (e.g.,
several orders of magnitude) in the process <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>
<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. Having been trained on data produced by actual runs of a given
HPC simulation, such a surrogate can imitate, with high fidelity, part
or all of that simulation, producing the same outcomes for a given set
of inputs, but at far less cost in time and energy.</p><img src=/images/sbi-fair-arch.png><p>Figure 1. The Surrogate Benchmark Initiative (SBI)and its components</p><p>As a world leader in HPC for many decades, the Department of Energy
will undoubtedly seek to exploit the power of such AI-driven
surrogates, especially because of the end of Dennard scaling and
Moore’s law. However, at present, there are no accepted benchmarks for
such surrogates, and so no way to measure progress or inform the
codesign of new HPC systems to support their use. The Surrogate
Benchmark Initiative (SBI) project proposed below aims to address this
fundamental problem by creating a community repository and FAIR data
ecosystem for HPC application surrogate benchmarks, including data,
code, and all relevant collateral artifacts the science and
engineering community needs to use and reuse these data sets and
surrogates.</p><p>To make “&mldr; scientific data publicly available to the AI community so
that algorithms, tools, and techniques work for science,” we propose a
community-driven, FAIR benchmarking activity that will 1) support AI
research into different attractive approaches and 2) provide exemplars
with reference implementations that will enable surrogates to be
extended across a wide range of scientific fields, while encompassing
the many different aspects of simulation where surrogates are
useful. The key components of the project are depicted in Figure 1
above.</p><p>By collaborating with the major industry organization in this area -
MLPerf and mirroring their process as much as possible, we will both
increase the value of and obtain industry involvement in the SBI
benchmarks. MLPerf has over 80 institutional members (mainly from
industry) and strong existing involvement of the Department of Energy
laboratories through the HPC working group inside MLPerf, which is now
being extended with a science data working group. To ensure that FAIR
principles are rigorously followed, we will initially set up data and
model repositories outside MLPerf. Containers and service
specifications such as OpenAPI will be systematically used. We will
then explore how much can be usefully and FAIRly integrated with
MLPerf, as our repositories have related but different goals and
constraints from MLPerf. To learn how to effectively and efficiently
set up FAIR repositories, we will start with (updates of) existing
surrogates from team members.</p><p>Simultaneously, we will reach out to the community of experienced
users building on our recent review <sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> and recent papers <sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>,
<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. The outreach will use permanent SBI working groups with the
Zoom/Meet/Teams/BlueJeans/Slack/cloud support that is now common and
these will link to appropriate MLPerf groups. Online tutorials will be
constructed based on the data and AI models that will support the
broad understanding of the use and design of surrogates. These
tutorials will also be designed so that they can help other
stakeholders that need to understand the value of and requirements for
surrogates; this includes the systems software/middleware and hardware
architecture communities. The tutorials will be an early goal so we
can reach out to domain scientists with important simulation codes but
so far little or unsophisticated surrogate use.</p><p>A key aspect of SBI will be the development of an efficient generic
surrogate architecture and accompanying middleware that will support
the derivation and use of surrogates across many fields. Another
specific activity will be the support of the use of benchmarks in the
uncertainty quantification of the surrogate estimates. Thirdly there
will be important studies of the amount of training data needed to get
reliable surrogates for a given accuracy choice. We have already
developed an effective performance model for surrogates but this needs
extension as deeper uses of surrogates become understood and populated
in our repositories.</p><p>We will link the repositories to important hardware systems including
major DoE and NSF environments, commercial high-performance clouds,
and available novel hardware. The study of the emerging AI systems
space is an important goal of our project as our benchmarks stress
both AI and simulation performance and so may not give the same
conclusions as purely AI-focused benchmarks. Although we initially
stress simulation surrogates, we will also consider AI surrogates for
big data computations.</p><p>We intend that our repositories will generate active research from
both the participants in our project and the broad community of AI and
domain scientists. The FAIR ease of use, tutorials, and links to
relevant execution platforms will be important. To initiate and foster
strong virtual community support we will also use hackathons, Meetups,
journal special issues, conference tutorials, and exhibits to nurture
the outside use of our resources. As well as advancing research, which
is our focus, we expect the project will be valuable for education and
training. The project will explicitly fund staff to make sure that
non-project users are properly supported and that our use of FAIR
principles is effective.</p><h2 id=refernces>Refernces<a class=td-heading-self-link href=#refernces aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Geoffrey Fox, Shantenu Jha, “Understanding ML driven HPC:
Applications and Infrastructure,” in IEEE eScience 2019
Conference, San Diego, California [Online]. Available:
<a href=https://escience2019.sdsc.edu/>https://escience2019.sdsc.edu/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Geoffrey Fox, Shantenu Jha, “Learning Everywhere: A Taxonomy for
the Integration of Machine Learning and Simulations,” in IEEE
eScience 2019 Conference, San Diego, California
[Online]. Available: <a href=https://arxiv.org/abs/1909.13340>https://arxiv.org/abs/1909.13340</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Geoffrey Fox, James A. Glazier, JCS Kadupitiya, Vikram Jadhao,
Minje Kim, Judy Qiu, James P. Sluka, Endre Somogyi, Madhav
Marathe, Abhijin Adiga, Jiangzhuo Chen, Oliver Beckstein, and
Shantenu Jha, “Learning Everywhere: Pervasive Machine Learning
for Effective High-Performance Computation,” in HPDC Workshop at
IPDPS 2019, Rio de Janeiro, 2019 [Online]. Available:
<a href=https://arxiv.org/abs/1902.10810>https://arxiv.org/abs/1902.10810</a>,
<a href=http://dsc.soic.indiana.edu/publications/Learning_Everywhere_Summary.pdf>http://dsc.soic.indiana.edu/publications/Learning_Everywhere_Summary.pdf</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>M. F. Kasim, D. Watson-Parris, L. Deaconu, S. Oliver,
P. Hatfield, D. H. Froula, G. Gregori, M. Jarvis, S. Khatiwala,
J. Korenaga, J. Topp-Mugglestone, E. Viezzer, and S. M. Vinko,
“Up to two billion times acceleration of scientific simulations
with deep neural architecture search,” arXiv [stat.ML],
17-Jan-2020 [Online]. Available: <a href=http://arxiv.org/abs/2001.08055>http://arxiv.org/abs/2001.08055</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>JCS Kadupitiya , Geoffrey C. Fox , and Vikram Jadhao, “Machine
learning for performance enhancement of molecular dynamics
simulations,” in International Conference on Computational
Science ICCS2019, Faro, Algarve, Portugal, 2019
[Online]. Available:
<a href=http://dsc.soic.indiana.edu/publications/ICCS8.pdf>http://dsc.soic.indiana.edu/publications/ICCS8.pdf</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>A. Moradzadeh and N. R. Aluru, “Molecular Dynamics Properties
without the Full Trajectory: A Denoising Autoencoder Network for
Properties of Simple Liquids,” J. Phys. Chem. Lett., vol. 10,
no. 24, pp. 7568–7576, Dec. 2019 [Online]. Available:
<a href=http://dx.doi.org/10.1021/acs.jpclett.9b02820>http://dx.doi.org/10.1021/acs.jpclett.9b02820</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Y. Sun, R. F. DeJaco, and J. I. Siepmann, “Deep neural network
learning of complex binary sorption equilibria from molecular
simulation data,” Chem. Sci., vol. 10, no. 16, pp. 4377–4388,
Apr. 2019 [Online]. Available:
<a href=http://dx.doi.org/10.1039/c8sc05340e>http://dx.doi.org/10.1039/c8sc05340e</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>F. Häse, I. Fdez Galván, A. Aspuru-Guzik, R. Lindh, and
M. Vacher, “How machine learning can assist the interpretation
of ab initio molecular dynamics simulations and conceptual
understanding of chemistry,” Chem. Sci., vol. 10, no. 8,
pp. 2298–2307, Feb. 2019 [Online]. Available:
<a href=http://dx.doi.org/10.1039/c8sc04516j>http://dx.doi.org/10.1039/c8sc04516j</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>O. Obiols-Sales, A. Vishnu, N. Malaya, and
A. Chandramowlishwaran, “CFDNet: a deep learning-based
accelerator for fluid simulations,” arXiv
[physics.flu-dyn]. 2020 [Online]. Available:
<a href=http://arxiv.org/abs/2005.04485>http://arxiv.org/abs/2005.04485</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>J. A. Tallman, M. Osusky, N. Magina, and E. Sewall, “An
Assessment of Machine Learning Techniques for Predicting
Turbine Airfoil Component Temperatures, Using FEA Simulations
for Training Data,” in ASME Turbo Expo 2019: Turbomachinery
Technical Conference and Exposition, 2019 [Online]. Available:
<a href=https://asmedigitalcollection.asme.org/GT/proceedings-abstract/GT2019/58646/V05AT20A002/1066873>https://asmedigitalcollection.asme.org/GT/proceedings-abstract/GT2019/58646/V05AT20A002/1066873</a>.
[Accessed: 23-Feb-2020]&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-89caff5907aed978dfb4a280efff213c>3 - Metadata Subgroup</h1><div class=lead>Metadata subgroup informatin</div><p>This subgroup is lead but University of Tennessee, Knoxville.</p><h1 id=schema-development>Schema Development<a class=td-heading-self-link href=#schema-development aria-label="Heading self-link"></a></h1><p>As part of the logging, reporting activities, this subgroup is tasked to create
appropriate schema to follow the FAIR principles. Below is a general overview
of the major hierarchy of data that needs to be recorded for reproducibility.</p><ul><li>Hardware specifications<ul><li>Compute: CPUs, Accelerators</li><li>Memory: caches, NUMA</li><li>Network: on-node CPU and accelerator coherency, NIC and off-node switches</li><li>Peripherals</li><li>Storage: primary (SSD), secondary (HDD), tertiary (RAID/remote)</li><li>Firmware: ID/release date</li></ul></li><li>Software stack<ul><li>Compiler: GCC, Clang, vendor</li><li>AI framework: TensorFlow, PyTorch, Keras, MxNet</li><li>Tensor backend: JAX, TVM</li><li>Runtime: JVM, OpenMP, CUDA</li><li>Messaging API: MPI, NCCL, RCCL</li><li>OS: Linux</li><li>Container: Singularity, Docker, CharlieCloud</li></ul></li><li>Input data<ul><li>Data sets (version, size)<ul><li>Image: MNIST digits/fashion, CIFAR 10/100, ImageNet, VGG</li><li>Language: Transformer</li><li>Science: instrument, simulation</li></ul></li><li>Annotations</li></ul></li><li>Model data<ul><li>Release date, ID, repo/branch/tag/hash, URL</li></ul></li><li>Output data<ul><li>Performance rate: training, inference</li><li>Power draw: training, inference</li><li>Energy consumption</li><li>Convergence: epochs</li><li>Accuracy, recall</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b764d436041462d3a04ca39c7db93384>4 - Publications</h1><div class=lead>We list here the Publications of this project</div><div class="pageinfo pageinfo-primary"><p>The collection of publications related to this project.</p></div><ul><li><strong>Note:</strong> <em>Please do not edit this page as it is automatically generated. To add new refernces please edit the <a href=https://github.com/sbi-fair/sbi-fair.github.io/blob/main/content/en/docs/Publications/refs.bib>bibtex file</a></em></li></ul><p><span class=csl-left-margin>[1]
</span><span class=csl-right-inline>G. Fox, P. Beckman, S. Jha, P.
Luszczek, and V. Jadhao, “Surrogate benchmark initiative SBI: FAIR
surrogate benchmarks supporting AI and simulation research,” in <em>ASCR
computer science (CS) principal investigators (PI) meeting</em>, Atlanta,
GA: U.S. Department of Energy (DOE), Office of Science (SC), Feb. 2024,
p. 1. Available:
<a href=https://github.com/sbi-fair/sbi-fair.github.io/raw/main/pub/doe_abstract.pdf>https://github.com/sbi-fair/sbi-fair.github.io/raw/main/pub/doe_abstract.pdf</a></span></p><p><span class=csl-left-margin>[2]
</span><span class=csl-right-inline>T. Zhong, J. Zhao, X. Guo, Q. Su,
and G. Fox, “RINAS: Training with dataset shuffling can be general and
fast.” 2023. Available: <a href=https://arxiv.org/abs/2312.02368>https://arxiv.org/abs/2312.02368</a></span></p><p><span class=csl-left-margin>[3]
</span><span class=csl-right-inline>C. Luo, T. Zhong, and G. Fox,
“RTP: Rethinking tensor parallelism with memory deduplication.” 2023.
Available: <a href=https://arxiv.org/abs/2311.01635>https://arxiv.org/abs/2311.01635</a></span></p><p><span class=csl-left-margin>[4]
</span><span class=csl-right-inline>“Quadri-partite quantum-assisted
VAE as a calorimeter surrogate,” in <em>Bulletin of the american physical
society</em>, in APS march meeting. American Physical Society Sites.
Available: <a href=https://meetings.aps.org/Meeting/MAR24/Session/Y50.5>https://meetings.aps.org/Meeting/MAR24/Session/Y50.5</a></span></p><p><span class=csl-left-margin>[5]
</span><span class=csl-right-inline>J. Q. Toledo-Marín, G. Fox, J. P.
Sluka, and J. A. Glazier, “Deep learning approaches to surrogates for
solving the diffusion equation for mechanistic real-world simulations.”
2021. Available: <a href=https://arxiv.org/abs/2102.05527>https://arxiv.org/abs/2102.05527</a></span></p><p><span class=csl-left-margin>[6]
</span><span class=csl-right-inline>J. Q. Toledo-Marín, G. Fox, J. P.
Sluka, and J. A. Glazier, “Deep learning approaches to surrogates for
solving the diffusion equation for mechanistic real-world simulations,”
<em>Frontiers in Physiology</em>, vol. 12, 2021, doi:
<a href=https://doi.org/10.3389/fphys.2021.667828>10.3389/fphys.2021.667828</a>.</span></p><p><span class=csl-left-margin>[7]
</span><span class=csl-right-inline>J. Kadupitiya, F. Sun, G. Fox, and
V. Jadhao, “Machine learning surrogates for molecular dynamics
simulations of soft materials,” <em>Journal of Computational Science</em>, vol.
42, p. 101107, 2020, Available:
<a href=https://par.nsf.gov/servlets/purl/10188151>https://par.nsf.gov/servlets/purl/10188151</a></span></p><p><span class=csl-left-margin>[8]
</span><span class=csl-right-inline>V. Jadhao and J. Kadupitiya,
“Integrating machine learning with hpc-driven simulations for enhanced
student learning,” in <em>2020 IEEE/ACM workshop on education for
high-performance computing (EduHPC)</em>, IEEE, 2020, pp. 25–34. Available:
<a href=https://api.semanticscholar.org/CorpusID:221376417>https://api.semanticscholar.org/CorpusID:221376417</a></span></p><p><span class=csl-left-margin>[9]
</span><span class=csl-right-inline>A. Clyde <em>et al.</em>, “Protein-ligand
docking surrogate models: A SARS-CoV-2 benchmark for deep learning
accelerated virtual screening.” 2021. Available:
<a href=https://arxiv.org/abs/2106.07036>https://arxiv.org/abs/2106.07036</a></span></p><p><span class=csl-left-margin>[10]
</span><span class=csl-right-inline>E. A. Huerta <em>et al.</em>, “FAIR for
AI: An interdisciplinary and international community building
perspective,” <em>Scientific Data</em>, vol. 10, no. 1, p. 487, 2023,
Available: <a href=https://doi.org/10.1038/s41597-023-02298-6>https://doi.org/10.1038/s41597-023-02298-6</a></span></p><p><span class=csl-left-margin>[11]
</span><span class=csl-right-inline>G. von Laszewski, J. P. Fleischer,
and G. C. Fox, “Hybrid reusable computational analytics workflow
management with cloudmesh.” 2022. Available:
<a href=https://arxiv.org/abs/2210.16941>https://arxiv.org/abs/2210.16941</a></span></p><p><span class=csl-left-margin>[12]
</span><span class=csl-right-inline>V. Chennamsetti <em>et al.</em>,
“MLCommons cloud masking benchmark with early stopping.” 2023.
Available: <a href=https://arxiv.org/abs/2401.08636>https://arxiv.org/abs/2401.08636</a></span></p><p><span class=csl-left-margin>[13]
</span><span class=csl-right-inline>G. von Laszewski and R. Gu, “An
overview of MLCommons cloud mask benchmark: Related research and data.”
2023. Available: <a href=https://arxiv.org/abs/2312.04799>https://arxiv.org/abs/2312.04799</a></span></p><p><span class=csl-left-margin>[14]
</span><span class=csl-right-inline>G. von Laszewski <em>et al.</em>,
“Whitepaper on reusable hybrid and multi-cloud analytics service
framework.” 2023. Available: <a href=https://arxiv.org/abs/2310.17013>https://arxiv.org/abs/2310.17013</a></span></p><p><span class=csl-left-margin>[15]
</span><span class=csl-right-inline>G. von Laszewski, J. P. Fleischer,
G. C. Fox, J. Papay, S. Jackson, and J. Thiyagalingam, “Templated hybrid
reusable computational analytics workflow management with cloudmesh,
applied to the deep learning MLCommons cloudmask application,” in
<em>eScience’23</em>, Limassol, Cyprus: Second Workshop on Reproducible
Workflows, Data,; Security (ReWorDS 2022), 2023. Available:
<a href=https://github.com/cyberaide/paper-cloudmesh-cc-ieee-5-pages/raw/main/vonLaszewski-cloudmesh-cc.pdf>https://github.com/cyberaide/paper-cloudmesh-cc-ieee-5-pages/raw/main/vonLaszewski-cloudmesh-cc.pdf</a></span></p><p><span class=csl-left-margin>[16]
</span><span class=csl-right-inline>G. von Laszewski <em>et al.</em>,
“Opportunities for enhancing MLCommons efforts while leveraging insights
from educational MLCommons earthquake benchmarks efforts,” <em>Frontiers in
High Performance Computing,</em> vol. 1, no. 1233877, p. 31, 2023,
Available: <a href=https://doi.org/10.3389/fhpcp.2023.1233877>https://doi.org/10.3389/fhpcp.2023.1233877</a></span></p><p><span class=csl-left-margin>[17]
</span><span class=csl-right-inline>G. von Laszewski, “Cloudmesh.” Web
Page, Jan. 2024. Available:
<a href=https://github.com/orgs/cloudmesh/repositories>https://github.com/orgs/cloudmesh/repositories</a></span></p><p><span class=csl-left-margin>[18]
</span><span class=csl-right-inline>G. von Laszewski, “Reusable hybrid
and multi-cloud analytics service framework,” in <em>4th international
conference on big data, IoT, and cloud computing (ICBICC 2022)</em>,
Chengdu, China: IASED, 2022. Available:
<a href=https://www.icbicc.org>www.icbicc.org</a></span></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4b7b381f29076b85f3dfb64e89a089c1>5 - Team</h1><div class=lead>The team members of the project</div><ul><li>Geoffrey Fox, Indiana University (Principal Investigator)</li><li>Vikram Jadhao, Indiana University (Co-Investigator)</li><li>Gregor von Laszewski, Indiana University (Co-Investigator),
<a href=mailto:laszewski@gmail.com>laszewski@gmail.com</a>, <a href=https://laszewski.github.io>https://laszewski.github.io</a></li><li>Rick Stevens, Argonne National Laboratory (Co-Investigator)</li><li>Peter Beckman, Argonne National Laboratory (Co-Investigator)</li><li>Kamil Iskra, Argonne National Laboratory (Co-Investigator)</li><li>Min Si, Argonne National Laboratory (Co-Investigator)</li><li>Jack Dongarra, University of Tennessee, Knoxville (Co-Investigator)</li><li>Piotr Luszczek, University of Tennessee, Knoxville (Co-Investigator)</li><li>Shantenu Jha, Rutgers University (Co-Investigator)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bbcb72d0609a64a0b2c2b5b4b06fc741>6 - Surrogates</h1><div class=lead>A list of surrogates we look at</div><p>A list of surrogates</p></div><div class=td-content><h1 id=pg-541c846fdc6fb55135b27065f2879d73>6.1 - AutoPhaseNN: unsupervised physics-aware deep learning of 3D nanoscale Bragg coherent diffraction imaging</h1><div class=lead>A DL-based approach which learns to solve the phase problem in 3D X-ray Bragg coherent diffraction imaging (BCDI) without labeled data.</div><hr><h2 id=metadata>Metadata<a class=td-heading-self-link href=#metadata aria-label="Heading self-link"></a></h2><hr><p><strong>Model</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/a/autophasenn.json>autophasenn.json</a></p><p><strong>Datasets</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/a/autoPhaseNN_aicdi.json>autoPhaseNN_aicdi.json</a></p><hr><blockquote><p>Adapted from Yao, Y. <em>et. al</em> <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> under CC-BY <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><p>AutoPhaseNN <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, a physics-aware unsupervised deep convolutional neural network
(CNN) that learns to solve the phase problem without ever being shown real space
images of the sample amplitude or phase. By incorporating the physics of the
X-ray scattering into the network design and training, AutoPhaseNN learns to
predict both the amplitude and phase of the sample given the measured
diffraction intensity alone. Additionally, unlike previous deep learning models,
AutoPhaseNN does not need the ground truth images of sample’s amplitude and
phase at any point, either in training or in deployment. Once trained, the
physical model is discarded and only the CNN portion is needed which has learned
the data inversion from reciprocal space to real space and is ~100 times faster
than the iterative phase retrieval with comparable image quality. Furthermore,
we show that by using AutoPhaseNN’s prediction as the learned prior to iterative
phase retrieval, we can achieve consistently higher image quality, than neural
network prediction alone, at 10 times faster speed than iterative phase
retrieval alone.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:910px><img class=card-img-top src=/docs/surrogates/autophasenn/autophasenn1_huf90a121ae4129385ddbbadbc4dfa22cd_382578_900x0_resize_catmullrom_3.png width=900 height=429><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Fig. 1: Schematic of the neural network structure of AutoPhaseNN model during training.
a) The model consists of a 3D CNN and the X-ray scattering forward model. The 3D
CNN is implemented with a convolutional auto-encoder and two deconvolutional
decoders using the convolutional, maximum pooling, upsampling and zero padding
layers. The physical knowledge is enforced via the Sigmoid and Tanh activation
functions in the final layers. b The X-ray scattering forward model includes the
numerical modeling of diffraction and the image shape constraints. It takes the
amplitude and phase from the 3D CNN output to form the complex image. Then the
estimated diffraction pattern is obtained from the FT of the current estimation
of the real space image.
<small class=text-body-secondary><br>Image from: Yao, Y. et al / CC-BY</small></p></figcaption></figure><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Yao, Y., Chan, H., Sankaranarayanan, S. et al. AutoPhaseNN: unsupervised physics-aware deep learning of 3D nanoscale Bragg coherent diffraction imaging. npj Comput Mater 8, 124 (2022). <a href=https://doi.org/10.1038/s41524-022-00803-w>https://doi.org/10.1038/s41524-022-00803-w</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=http://creativecommons.org/licenses/by/4.0/>http://creativecommons.org/licenses/by/4.0/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e87c590decacea82502a53d7c30c5035>6.2 - Calorimeter surrogates</h1><div class=lead>The Kaggle calorimeter challenge uses generative AI to produce a surrogate for the Monte Carlo calculation of a calorimeter response to an incident particle (ATLAS data at LHC calculated with GEANT4).</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>The Kaggle calorimeter challenge uses generative AI to produce a
surrogate for the Monte Carlo calculation of a calorimeter response to
an incident particle (ATLAS data at LHC calculated with
GEANT4). Variational Auto Encoders, GANs, Normalizing Flows, and
Diffusion Models. We also have a surrogate using a Quantum Computer
(DWAVE) annealer to generate random samples. We have identified four
different surrogates that are available openly from Kaggle and later
submissions.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:938px><img class=card-img-top src=/docs/surrogates/calorimeter/caloremiter_hub1b4528fbc9f1add21b4ab579c06711a_1445091_928x710_fill_catmullrom_smart1_3.png width=928 height=710><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: CaloChallenge Dataset.</p></figcaption></figure><h2 id=details>Details<a class=td-heading-self-link href=#details aria-label="Heading self-link"></a></h2><p>Accurate simulation plays a crucial role in particle physics by
bridging theoretical models with experimental data to uncover the
universe&rsquo;s fundamental properties. At the Large Hadron Collider (LHC),
simulations based on Monte Carlo methods model the interactions of
billions of particles, including complex calorimeter shower
events—cascades of secondary particles produced when high-energy
particles hit detector materials. The widely-used Geant4
<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>
simulation toolkit provides highly detailed physics-based simulations,
but its computational cost is extremely high, making up over 75% of
the total simulation time
<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. With the upcoming High-Luminosity LHC
(HL-LHC)
<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup><sup>,</sup><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>
upgrade in 2029, the collider will generate larger
datasets with higher precision requirements, significantly increasing
the demand for computational resources. To mitigate this, researchers
are exploring generative models commonly used in image and text
generation—as surrogate models that can generate realistic calorimeter
showers at a fraction of the computational cost. In recent years,
several approaches based on Generative Adversarial Networks(GAN)
<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup><sup>,</sup>
<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup><sup>,</sup>
<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup><sup>,</sup>
<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup><sup>,</sup>
<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup><sup>,</sup>
<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup><sup>,</sup>
Diffusion
<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>
<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup><sup>,</sup>
<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup><sup>,</sup>
<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup><sup>,</sup>
<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup><sup>,</sup>
<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup><sup>,</sup>
<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup><sup>,</sup>
<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup><sup>,</sup>
<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>, Variational Autoencoders (VAEs)
<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup><sup>,</sup>
<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup><sup>,</sup>
<sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup><sup>,</sup>
<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup><sup>,</sup>
<sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup><sup>,</sup>
<sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup><sup>,</sup>
<sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup><sup>,</sup>
<sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup><sup>,</sup>
<sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>
and Normalizing Flows
<sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup><sup>,</sup>
<sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup><sup>,</sup>
<sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup><sup>,</sup>
<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup><sup>,</sup>
<sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup><sup>,</sup>
<sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup><sup>,</sup>
<sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup>
have been proposed. However, evaluating
these models remains challenging because the physical characteristics
of calorimeter showers differ significantly from traditional image-
and text-based data.
<sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup><sup>,</sup>
<sup id=fnref:37><a href=#fn:37 class=footnote-ref role=doc-noteref>37</a></sup> conducted a rigorous evaluation of these
generative models using standard datasets and a diverse set of metrics
derived from physics, computer vision, and statistics. Although
<sup id=fnref1:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup>
sheds light on the existent correlations between layers, they do not
quantify correlations between layers and voxels. In this work, we
propose Correlation Frobenius Distance (CFD), an evaluation metric for
generative models of calorimeter shower simulation. This metric
measures how the consecutive layers and voxels of generated samples
are correlated with each other compared to Geant4 samples. CFD helps
evaluate the consistency of energy deposition patterns across layers,
capturing the spatial correlations in the calorimeter shower. Lower
CFD values indicate that the generated samples better preserve the
correlations observed in Geant4 simulations. We compared four
different models (CaloDream
<sup id=fnref1:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>, CaloScore v2 <sup id=fnref1:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>, CaloDiffusion
<sup id=fnref1:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup>, and CaloINN
<sup id=fnref1:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup>) on Dataset 2
<sup id=fnref:38><a href=#fn:38 class=footnote-ref role=doc-noteref>38</a></sup> from CaloChallenge 2022
<sup id=fnref1:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> for CFD, our observation reveals that CaloDream can capture
correlations between consecutive layers and voxels the
best. Furthermore, we explored the impact of using full versus mixed
precision modes during inference for CaloDiffusion. Our observation
shows that mixed precision inference does not speed up inference for
Dataset 1
<sup id=fnref:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup> and Dataset 2
<sup id=fnref1:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>.​ However, it significantly improves inference time for Dataset 3
<sup id=fnref2:39><a href=#fn:39 class=footnote-ref role=doc-noteref>39</a></sup>, without compromising
performance.​ The Code is available in GitHub at
<sup id=fnref:40><a href=#fn:40 class=footnote-ref role=doc-noteref>40</a></sup>.</p><p>Additional relevant references include:</p><p><sup id=fnref:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup><sup>,</sup>
<sup id=fnref:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup><sup>,</sup></p><p>Team contributed refernces include</p><p><sup id=fnref:43><a href=#fn:43 class=footnote-ref role=doc-noteref>43</a></sup><sup>,</sup>
<sup id=fnref:44><a href=#fn:44 class=footnote-ref role=doc-noteref>44</a></sup><sup>,</sup>
<sup id=fnref:45><a href=#fn:45 class=footnote-ref role=doc-noteref>45</a></sup><sup>,</sup>
<sup id=fnref:46><a href=#fn:46 class=footnote-ref role=doc-noteref>46</a></sup><sup>,</sup></p><p><strong>References</strong></p><p>Team contributed refernces are marked in bold</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Agostinelli, Sea, et al. &ldquo;GEANT4—a simulation toolkit.&rdquo; <em>Nuclear instruments and methods in physics research section A: Accelerators, Spectrometers, Detectors and Associated Equipment</em> 506.3 (2003): 250-303.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Muškinja, Miha, John Derek Chapman, and Heather Gray. &ldquo;Geant4 performance optimization in the ATLAS experiment.&rdquo; <em>EPJ Web of Conferences</em>. Vol. 245. EDP Sciences, 2020.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>&ldquo;New Schedule for CERN&rsquo;s Accelerators.&rdquo; <em>CERN</em>, 5 Dec. 2023, [https://home.cern/news/news/accelerators/new-schedule-cerns-accelerators]:(<a href=https://home.cern/news/news/accelerators/new-schedule-cerns-accelerators)>https://home.cern/news/news/accelerators/new-schedule-cerns-accelerators)</a>. Accessed 28 Feb. 2025.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>&ldquo;Computing at CERN.&rdquo; <em>CERN</em>, <a href=https://home.web.cern.ch/science/computing>https://home.web.cern.ch/science/computing</a>. Accessed 28 Feb. 2025.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>ATLAS collaboration. &ldquo;Fast simulation of the ATLAS calorimeter system with Generative Adversarial Networks.&rdquo; <em>ATLAS PUB Note, CERN, Geneva</em> (2020).&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Ghosh, Aishik, and ATLAS collaboration. &ldquo;Deep generative models for fast shower simulation in ATLAS.&rdquo; <em>Journal of Physics: Conference Series</em>. Vol. 1525. No. 1. IOP Publishing, 2020.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Giannelli, Michele Faucci, and Rui Zhang. &ldquo;CaloShowerGAN, a generative adversarial network model for fast calorimeter shower simulation.&rdquo; <em>The European Physical Journal Plus</em> 139.7 (2024): 597.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Paganini, Michela, Luke de Oliveira, and Benjamin Nachman. &ldquo;Accelerating science with generative adversarial networks: an application to 3D particle showers in multilayer calorimeters.&rdquo; <em>Physical review letters</em> 120.4 (2018): 042003.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>de Oliveira, Luke, Michela Paganini, and Benjamin Nachman. &ldquo;Learning particle physics by example: location-aware generative adversarial networks for physics synthesis.&rdquo; <em>Computing and Software for Big Science</em> 1.1 (2017): 4.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Paganini, Michela, Luke de Oliveira, and Benjamin Nachman. &ldquo;CaloGAN: Simulating 3D high energy particle showers in multilayer electromagnetic calorimeters with generative adversarial networks.&rdquo; <em>Physical Review D</em> 97.1 (2018): 014021.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Acosta, Fernando Torales, et al. &ldquo;Comparison of point cloud and image-based models for calorimeter fast simulation.&rdquo; <em>Journal of Instrumentation</em> 19.05 (2024): P05003.&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Amram, Oz, and Kevin Pedro. &ldquo;Denoising diffusion models with geometry adaptation for high fidelity calorimeter simulation.&rdquo; <em>Physical Review D</em> 108.7 (2023): 072014.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Buhmann, Erik, et al. &ldquo;CaloClouds: fast geometry-independent highly-granular calorimeter simulation.&rdquo; <em>Journal of Instrumentation</em> 18.11 (2023): P11025.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Buhmann, Erik, et al. &ldquo;CaloClouds II: ultra-fast geometry-independent highly-granular calorimeter simulation.&rdquo; <em>Journal of Instrumentation</em> 19.04 (2024): P04020.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Cresswell, Jesse C., and Taewoo Kim. &ldquo;Scaling Up Diffusion and Flow-based XGBoost Models.&rdquo; <em>arXiv preprint arXiv:2408.16046</em> (2024).&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Madula, T., and V. M. Mikuni. &ldquo;CaloLatent: Score-based Generative Modelling in the Latent Space for Calorimeter Shower Generation NeurIPS Workshop on Machine Learning and the Physical Sciences URL https://ml4physicalsciences. github. io/2023/files.&rdquo; <em>NeurIPS_ ML4PS_2023_19. pdf</em> (2023).&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Mikuni, Vinicius, and Benjamin Nachman. &ldquo;Score-based generative models for calorimeter shower simulation.&rdquo; <em>Physical Review D</em> 106.9 (2022): 092009.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Mikuni, Vinicius, and Benjamin Nachman. &ldquo;CaloScore v2: single-shot calorimeter shower simulation with diffusion models.&rdquo; <em>Journal of Instrumentation</em> 19.02 (2024): P02001.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Favaro, Luigi, et al. &ldquo;CaloDREAM&ndash;Detector Response Emulation via Attentive flow Matching.&rdquo; <em>arXiv preprint arXiv:2405.09629</em> (2024).&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Cresswell, Jesse C., et al. &ldquo;CaloMan: Fast generation of calorimeter showers with density estimation on learned manifolds.&rdquo; <em>arXiv preprint arXiv:2211.15380</em> (2022).&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Buhmann, Erik, et al. &ldquo;Decoding photons: Physics in the latent space of a BIB-AE generative network.&rdquo; <em>EPJ Web of Conferences</em>. Vol. 251. EDP Sciences, 2021.&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Buhmann, Erik, et al. &ldquo;Getting high: High fidelity simulation of high granularity calorimeters with high speed.&rdquo; <em>Computing and Software for Big Science</em> 5.1 (2021): 13.&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>Diefenbacher, Sascha, et al. &ldquo;New angles on fast calorimeter shower simulation.&rdquo; <em>Machine Learning: Science and Technology</em> 4.3 (2023): 035044.&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Salamani, Dalila, Anna Zaborowska, and Witold Pokorski. &ldquo;MetaHEP: Meta learning for fast shower simulation of high energy physics experiments.&rdquo; <em>Physics Letters B</em> 844 (2023): 138079.&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Abhishek, Abhishek, et al. &ldquo;CaloDVAE: Discrete variational autoencoders for fast calorimeter shower simulation.&rdquo; <em>arXiv preprint arXiv:2210.07430</em> (2022).&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26><p>Caloqvae: Simulating high-energy particle calorimeter interactions using hybrid quantum-classical generative models&#160;<a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27><p>Hoque, Sehmimul, et al. &ldquo;CaloQVAE: Simulating high-energy particle-calorimeter interactions using hybrid quantum-classical generative models.&rdquo; <em>The European Physical Journal C</em> 84.12 (2024): 1-7.&#160;<a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28><p>Lu, Ian, et al. &ldquo;Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions.&rdquo; <em>arXiv preprint arXiv:2412.04677</em> (2024).&#160;<a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29><p>Krause, Claudius, and David Shih. &ldquo;Fast and accurate simulations of calorimeter showers with normalizing flows.&rdquo; <em>Physical Review D</em> 107.11 (2023): 113003.&#160;<a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30><p>Krause, Claudius, Ian Pang, and David Shih. &ldquo;CaloFlow for CaloChallenge dataset 1.&rdquo; <em>SciPost Physics</em> 16.5 (2024): 126.&#160;<a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31><p>Buckley, Matthew R., et al. &ldquo;Inductive simulation of calorimeter showers with normalizing flows.&rdquo; <em>Physical Review D</em> 109.3 (2024): 033006.&#160;<a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32><p>Diefenbacher, S., et al. &ldquo;L2LFlows: generating high-fidelity 3D calorimeter images (2023).&rdquo; <em>arXiv preprint arXiv:2302.11594</em> 18: P10017.&#160;<a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33><p>Ernst, Florian, et al. &ldquo;Normalizing flows for high-dimensional detector simulations.&rdquo; <em>arXiv preprint arXiv:2312.09290</em> (2023).&#160;<a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34><p>Liu, Junze, et al. &ldquo;Geometry-aware autoregressive models for calorimeter shower simulations.&rdquo; <em>arXiv preprint arXiv:2212.08233</em> (2022).&#160;<a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35><p>Schnake, Simon, Dirk Krücker, and Kerstin Borras. &ldquo;CaloPointFlow II generating calorimeter showers as point clouds.&rdquo; <em>arXiv preprint arXiv:2403.15782</em> (2024).&#160;<a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36><p><strong>Ahmad, Farzana Yasmin, Vanamala Venkataswamy, and Geoffrey Fox. &ldquo;A comprehensive evaluation of generative models in calorimeter shower simulation.&rdquo; <em>arXiv preprint arXiv:2406.12898</em> (2024).</strong>&#160;<a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:37><p>Krause, Claudius, et al. &ldquo;Calochallenge 2022: A community challenge for fast calorimeter simulation.&rdquo; <em>arXiv preprint arXiv:2410.21611</em> (2024).&#160;<a href=#fnref:37 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:38><p>Ahmad, F. Y. Generated Samples of Dataset 2 from Calochallenge_2022. Zenodo, 17 Feb. 2025, doi:10.5281/zenodo.14883798.&#160;<a href=#fnref:38 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:39><p>CaloChallenge Homepage*, calochallenge.github.io/homepage/. Accessed 3 Mar. 2025.&#160;<a href=#fnref:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:39 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:40><p>GitHub: <a href=https://github.com/Aaheer17/Benchmarking_Calorimeter_Shower_Simulation_Generative_AI/tree/main>https://github.com/Aaheer17/Benchmarking_Calorimeter_Shower_Simulation_Generative_AI/tree/main</a>&#160;<a href=#fnref:40 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:41><p>Michele Faucci Giannelli, Gregor Kasieczka, Claudius
Krause, Ben Nachman, Dalila Salamani, David Shih,
Anna Zaborowska, Fast calorimeter simulation challenge
2022 - dataset 1,2 and 3 [data set]. zenodo., <a href=https://doi.org/10.5281/zenodo.8099322>https://doi.org/10.5281/zenodo.8099322</a>, <a href=https://doi.org/10.5281/zenodo.6366271>https://doi.org/10.5281/zenodo.6366271</a>, <a href=https://doi.org/10.5281/zenodo.6366324>https://doi.org/10.5281/zenodo.6366324</a> (2022).&#160;<a href=#fnref:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:41 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:42><p>ATLAS Collaboration, ATLAS software and computing HL-LHC roadmap, Tech. Rep. (Technical report, CERN, Geneva. <a href=http://cds.cern.ch/record/2802918>http://cds.cern.ch/record/2802918</a>, 2022).&#160;<a href=#fnref:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:42 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:43><p><strong>Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions, J Quetzalcoatl Toledo-Marin, Sebastian Gonzalez, Hao Jia, Ian Lu, Deniz Sogutlu, Abhishek Abhishek, Colin Gay, Eric Paquet, Roger Melko, Geoffrey C Fox, Maximilian Swiatlowski, Wojciech Fedorko, 2024/10/30
arXiv preprint arXiv:2410.22870, Abstract:
Particle collisions at accelerators such as the Large Hadron Collider, recorded and analyzed by experiments such as ATLAS and CMS, enable exquisite measurements of the Standard Model and searches for new phenomena. Simulations of collision events at these detectors have played a pivotal role in shaping the design of future experiments and analyzing ongoing ones. However, the quest for accuracy in Large Hadron Collider (LHC) collisions comes at an imposing computational cost, with projections estimating the need for millions of CPU-years annually during the High Luminosity LHC (HL-LHC) run <sup id=fnref1:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup>. Simulating a single LHC event with Geant4 currently devours around 1000 CPU seconds, with simulations of the calorimeter subdetectors in particular imposing substantial computational demands <sup id=fnref2:42><a href=#fn:42 class=footnote-ref role=doc-noteref>42</a></sup>. To address this challenge, we propose a conditioned quantum-assisted deep generative model. Our model integrates a conditioned variational autoencoder (VAE) on the exterior with a conditioned Restricted Boltzmann Machine (RBM) in the latent space, providing enhanced expressiveness compared to conventional VAEs. The RBM nodes and connections are meticulously engineered to enable the use of qubits and couplers on D-Wave&rsquo;s Pegasus-structured \textit{Advantage} quantum annealer (QA) for sampling. We introduce a novel method for conditioning the quantum-assisted RBM using flux biases. We further propose a novel adaptive mapping to estimate the effective inverse temperature in quantum annealers. The effectiveness of our framework is illustrated using Dataset 2 of the CaloChallenge <sup id=fnref1:41><a href=#fn:41 class=footnote-ref role=doc-noteref>41</a></sup>.</strong>&#160;<a href=#fnref:43 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:44><p><strong>Calorimeter Surrogate Research, Geoffrey Fox University of Virginia, 2024
<a href=https://docs.google.com/document/d/19g0Avj9SYbVH7qSxoVUnnFKeGMuBdD9JCHVmBQB466M/>https://docs.google.com/document/d/19g0Avj9SYbVH7qSxoVUnnFKeGMuBdD9JCHVmBQB466M/</a></strong>&#160;<a href=#fnref:44 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:45><p><strong>Poster: <a href=https://drive.google.com/file/d/1PUiNDju_8N_wsDKI_W-g-jyCHb_5Hepo/>https://drive.google.com/file/d/1PUiNDju_8N_wsDKI_W-g-jyCHb_5Hepo/</a></strong>&#160;<a href=#fnref:45 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:46><p><strong>Extended abstract: Correlation Frobenius Distance: A Metric for Evaluating Generative Models in Calorimeter Shower Simulation, Farzana Yasmin Ahmada, Vanamala Venkataswamya, Geoffrey Fox, University of Virginia,
<a href=https://docs.google.com/document/d/1ndHkJY41_pHYZZne58B4_7HJQKTCxPzeMWVMJ0bsnOE>https://docs.google.com/document/d/1ndHkJY41_pHYZZne58B4_7HJQKTCxPzeMWVMJ0bsnOE</a></strong>&#160;<a href=#fnref:46 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-386ca1030e8123ddf937ecb1eac2905c>6.3 - Virtual tissue</h1><div class=lead>This surrugate simulates a virtual tissue</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Neural networks (NNs) have been demonstrated to be a viable
alternative to traditional direct numerical evaluation algorithms,
with the potential to accelerate computational time by several orders
of magnitude. In the present paper we study the use of encoder-decoder
convolutional neural network (CNN) algorithms as surrogates for
steady-state diffusion solvers. The construction of such surrogates
requires the selection of an appropriate task, network architecture,
training set structure and size, loss function, and training algorithm
hyperparameters. It is well known that each of these factors can have
a significant impact on the performance of the resultant model. Our
approach employs an encoder-decoder CNN architecture, which we posit
is particularly wellsuited for this task due to its ability to
effectively transform data, as opposed to merely compressing it. We
systematically evaluate a range of loss functions, hyperparameters,
and training set sizes. Our results indicate that increasing the size
of the training set has a substantial effect on reducing performance
fluctuations and overall error. Additionally, we observe that the
performance of the model exhibits a logarithmic dependence on the
training set size. Furthermore, we investigate the effect on model
performance by using different subsets of data with varying
features. Our results highlight the importance of sampling the
configurational space in an optimal manner, as this can have a
significant impact on the performance of the model and the required
training time. In conclusion, our results suggest that training a
model with a pre-determined error performance bound is not a viable
approach, as it does not guarantee that edge cases with errors larger
than the bound do not exist. Furthermore, as most surrogate tasks
involve a high dimensional landscape, an ever increasing training set
size is, in principle, needed, however it is not a practical solution.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:959px><img class=card-img-top src=/docs/surrogates/virtualtissue/virtualtissue_hu4ece9ae9fa528d24b54d235d7699b339_80460_949x300_fill_catmullrom_smart1_3.png width=949 height=300><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: Sketch of the NN architecture for virtual tissue surrogate.</p></figcaption></figure><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup>,</sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Analyzing the Performance of Deep Encoder-Decoder Networks as Surrogates for a Diffusion Equation, J. Quetzalcoatl Toledo-Marin, James A. Glazier, Geoffrey Fox
<a href=https://arxiv.org/pdf/2302.03786.pdf>https://arxiv.org/pdf/2302.03786.pdf</a>>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>There is an earlier surrogate referred to in this arxiv. It was published:
Toledo-Marín J. Quetzalcóatl , Fox Geoffrey , Sluka James P. , Glazier James A.,
Deep Learning Approaches to Surrogates for Solving the Diffusion Equation for Mechanistic Real-World Simulations,Frontiers in Physiology, Vol. 12, 2021
doi: 10.3389/fphys.2021.667828,
ISSNI 1664-042X,
<a href=https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2021.667828>https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2021.667828</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c81d71fde3c0ee4014394d74adc53de0>6.4 - Cosmoflow</h1><div class=lead>The CosmoFlow training application benchmark from the MLPerf HPC v0.5 benchmark suite. It involves training a 3D convolutional neural network on N-body cosmology simulation data to predict physical parameters of the universe.</div><hr><h2 id=metadata>Metadata<a class=td-heading-self-link href=#metadata aria-label="Heading self-link"></a></h2><hr><p><strong>Model</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/c/cosmoflow.json>cosmoflow.json</a></p><p><strong>Datasets</strong></p><p><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/c/cosmoUniverse_2019_05_4parE_tf_v2.json>cosmoUniverse_2019_05_4parE_tf_v2.json</a></p><p><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/c/cosmoUniverse_2019_05_4parE_tf_v2_mini.json>cosmoUniverse_2019_05_4parE_tf_v2_mini.json</a></p><hr><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>This application is based on the original CosmoFlow paper presented at SC18 and continued by the ExaLearn project, and adopted as a benchmark in the MLPerf HPC suite. It involves training a 3D convolutional neural network on N-body cosmology simulation data to predict physical parameters of the universe. The reference implementation for MLPerf HPC v0.5 CosmoFlow uses TensorFlow with the Keras API and Horovod for data-parallel distributed training. The dataset comes from simulations run by ExaLearn, with universe volumes split into cubes of size 128x128x128 with 4 redshift bins. The total dataset volume preprocessed for MLPerf HPC v0.5 in TFRecord format is 5.1 TB. The target objective in MLPerf HPC v0.5 is to train the model to a validation mean-average-error &lt; 0.124. However, the problem size can be scaled down and the training throughput can be used as the primary objective for a small scale or shorter timescale benchmark.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:938px><img class=card-img-top src=/docs/surrogates/cosmoflow/cosmoflow_hub95de84566b8bf0f0a840dc39a9b7150_118291_928x900_fill_q75_catmullrom_smart1.jpg width=928 height=900><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: Example simulation of dark matter in the universe used as input to the CosmoFlow network.
Copied from [NERSC](https://www.nersc.gov/news-publications/nersc-news/science-news/2018/nersc-intel-cray-harness-the-power-of-deep-learning-to-better-understand-the-universe/)</p></figcaption></figure><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://proxyapps.exascaleproject.org/app/mlperf-cosmoflow/>https://proxyapps.exascaleproject.org/app/mlperf-cosmoflow/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://github.com/sparticlesteve/cosmoflow-benchmark>https://github.com/sparticlesteve/cosmoflow-benchmark</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/sparticlesteve/cosmoflow-benchmark/blob/master/README.md>https://github.com/sparticlesteve/cosmoflow-benchmark/blob/master/README.md</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f33b00fce1cb69e54b44c3dacba3bfad>6.5 - Fully ionized plasma fluid model closures</h1><div class=lead>The closure problem in fluid modeling is a well-known challenge to modelers aiming to accurately describe their system of interest. We will choose one of the surrogates form this application and develop a reference implementation and tutorial.</div><p>Fully ionized plasma fluid model closures (Argonne):<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> The closure
problem in fluid modeling is a well-known challenge to modelers aiming
to accurately describe their system of interest. Analytic formulations
in a wide range of regimes exist but a practical, generalized fluid
closure for magnetized plasmas remains an elusive goal. There are
scenarios where complex physics prevents a simple closure being
assumed, and the question as to what closure to employ has a
non-trivial answer. In a proof-of-concept study, Argonne researchers
turned to machine learning to try to construct surrogate closure
models that map the known macroscopic variables in a fluid model to
the higher-order moments that must be closed. In their study, the
researchers considered three closures: Braginskii, Hammett-Perkins,
and Guo-Tang; for each of them, they tried three types of ANNs:
locally connected, convolutional, and fully connected. Applying a
physics-informed machine learning approach, they found that there is a
benefit to tailoring a specific network architecture informed by the
physics of the plasma regime each closure is designed for, rather than
carelessly applying an unnecessarily complex general network
architecture. will choose one of the surrogates and bring it up an
early example for SBI with reference implementation and tutorial
documentation. As a follow-up, the Argonne team will tackle more
challenging problems.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:1610px><img class=card-img-top src=/docs/surrogates/ionized-plasma/plasma_huf5a4cf579253a6a452cc06a8d2bdd748_201640_1600x910_fill_catmullrom_smart1_3.png width=1600 height=910><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: Simple schematic of varying classes of closure formulations.</p></figcaption></figure><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>R. Maulik, N. A. Garland, X.-Z. Tang, and P. Balaprakash,
“Neural network representability of fully ionized plasma fluid
model closures,” arXiv [physics.comp-ph], 10-Feb-2020
[Online]. Available: <a href=http://arxiv.org/abs/2002.04106>http://arxiv.org/abs/2002.04106</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-78e52a1336c6e0869c51f7cf3eef7d30>6.6 - Ions in nanoconfinement</h1><div class=lead>This application studies ionic structure in electrolyte solutions in nanochannels with planar uncharged surfaces and can use multiple molecular dynamics (MD) codes including LAMMPS which run on HPC supercomputers with OpenMP and MPI parallelization.</div><h2 id=metadata>Metadata<a class=td-heading-self-link href=#metadata aria-label="Heading self-link"></a></h2><hr><p><strong>Model</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/n/nanoconfinement.json>nanoconfinement.json</a></p><p><strong>Datasets</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/n/nanoconfinement.json>nanoconfinement.json</a></p><hr><p>This application <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> studies
ionic structure in electrolyte solutions in nanochannels with planar
uncharged surfaces and can use multiple molecular dynamics (MD) codes
including LAMMPS <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> which run on HPC supercomputers with OpenMP and
MPI parallelization.</p><p>A dense neural-net (NN) was used to learn 150 final state
characteristics based on the input of 5 parameters with typical
results shown in fig. 2(b) with the NN results for three important
densities tracking well the MD simulation results for a wide range of
unseen input system parameters. Fig. 3(a,b) shows two typical density
profiles with again the NN prediction tracking well the
simulation. Input quantities were confinement length, positive ion
valency, negative ion valency, salt concentration, and ion
diameter. Figure 2(a) shows the runtime architecture for dynamic use
and update of the NN and our middleware discussed in Sec. 3.2.6 will
generalize this. The inference time for this on a single core is 104
times faster than the parallel code which is itself 100 times the
sequential code. This surrogate approach is the first-of-its-kind in
the area of simulating charged soft-matter systems and there are many
other published papers in both biomolecular and material science
presenting similar successful surrogates <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> with a NN architecture
similar to fig. 3(c).</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:935px><img class=card-img-top src=/docs/surrogates/ions-in-nanoconfinement/featured-ion1_hu54f46d044c4e12dbdf720fb8ecb29dcc_251133_925x254_fill_catmullrom_smart1_3.png width=925 height=254><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Fig. 2 a) Architecture of dynamic training of ML surrogate and b)
Comparison of three final state densities (peak, contact, and center)
between MD simulations and NN surrogate predictions [^5] [^51].</p></figcaption></figure><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:948px><img class=card-img-top src=/docs/surrogates/ions-in-nanoconfinement/featured-ion2_hu04e0f7269f76594d4a25e14f4e4eee06_144655_938x222_fill_catmullrom_smart1_3.png width=938 height=222><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Fig. 3 (a,b) Two density profiles of confined ions for very different
input parameters and comparing MD and NN. (c) Fully connected deep
learning network used to learn the final densities. ReLU activation
units are in the 512 and 256 node hidden layers. The output values
were learned on 150 nodes.</p></figcaption></figure><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>JCS Kadupitiya , Geoffrey C. Fox , and Vikram Jadhao, “Machine
learning for performance enhancement of molecular dynamics
simulations,” in International Conference on Computational
Science ICCS2019, Faro, Algarve, Portugal, 2019
[Online]. Available:
<a href=http://dsc.soic.indiana.edu/publications/ICCS8.pdf>http://dsc.soic.indiana.edu/publications/ICCS8.pdf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>J. C. S. Kadupitiya, F. Sun, G. Fox, and V. Jadhao, “Machine
learning surrogates for molecular dynamics simulations of soft
materials,” J. Comput. Sci., vol. 42, p. 101107, Apr. 2020
[Online]. Available:
<a href=http://www.sciencedirect.com/science/article/pii/S1877750319310609>http://www.sciencedirect.com/science/article/pii/S1877750319310609</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>“Molecular Dynamics for Nanoconfinement.” [Online]. Available:
<a href=https://github.com/softmaterialslab/nanoconfinement-md>https://github.com/softmaterialslab/nanoconfinement-md</a>. [Accessed: 11-May-2020]&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>S. Plimpton, “Fast Parallel Algorithms for Short Range
Molecular Dynamics,” J. Comput. Phys., vol. 117, pp. 1–19, 1995
[Online]. Available:
<a href=http://faculty.chas.uni.edu/~rothm/Modeling/Parallel/Plimpton.pdf>http://faculty.chas.uni.edu/~rothm/Modeling/Parallel/Plimpton.pdf</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Geoffrey Fox, Shantenu Jha, “Learning Everywhere: A Taxonomy for
the Integration of Machine Learning and Simulations,” in IEEE
eScience 2019 Conference, San Diego, California
[Online]. Available: <a href=https://arxiv.org/abs/1909.13340>https://arxiv.org/abs/1909.13340</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-da679368f9d15b0486abed0d11a2ce9a>6.7 - miniWeatherML</h1><div class=lead>A simplified weather model simulating flows such as supercells that are realistic enough to be challenging and simple enough for rapid prototyping in creating and learning about surrogates.</div><hr><h2 id=metadata>Metadata<a class=td-heading-self-link href=#metadata aria-label="Heading self-link"></a></h2><hr><p><strong>Model</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/m/miniWeatherML.json>miniWeatherML.json</a></p><p><strong>Datasets</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/m/miniWeatherML.json>miniWeatherML.json</a></p><hr><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>MiniWeatherML is a playground for learning and developing Machine Learning (ML) surrogate models and workflows. It is based on a simplified weather model simulating flows such as supercells that are realistic enough to be challenging and simple enough for rapid prototyping in:</p><ul><li>Data generation and curation</li><li>Machine Learning model training</li><li>ML model deployment and analysis</li><li>End-to-end workflows</li></ul><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:510px><img class=card-img-top src=/docs/surrogates/miniweatherml/weather_hu5e8599bfb2e591cc67963ae5090a8232_2812795_500x500_fill_catmullrom_smart1_1.gif width=500 height=500><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: CANcer Distributed Learning Environment</p></figcaption></figure><img src=https://camo.githubusercontent.com/f7d00138e4e45ee24367fc16bc6f7de194c1c5a1ceebb03f85fb0c1cdcc4e314/68747470733a2f2f6d726e6f726d616e2e6769746875622e696f2f737570657263656c6c5f6d696e69576561746865724d4c2e676966><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>,<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/mrnorman/miniWeatherML>https://github.com/mrnorman/miniWeatherML</a><sup>,</sup>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://github.com/mrnorman/miniWeatherML/wiki>https://github.com/mrnorman/miniWeatherML/wiki</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6e66623ebf80ce302fd399a8dd6f8d7b>6.8 - OSMI</h1><div class=lead>We explore the relationship between certain network configurations and the performance of distributed Machine Learning systems. We build upon the Open Surrogate Model Inference (OSMI) Benchmark, a distributed inference benchmark for analyzing the performance of machine-learned surrogate models</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>We explore the relationship between certain network configurations and
the performance of distributed Machine Learning systems. We build upon
the Open Surrogate Model Inference (OSMI) Benchmark, a distributed
inference benchmark for analyzing the performance of machine-learned
surrogate models developed by Wes Brewer et. Al. We focus on analyzing
distributed machine-learning systems, via machine-learned surrogate
models, across varied hardware environments. By deploying the OSMI
Benchmark on platforms like Rivanna HPC, WSL, and Ubuntu, we offer a
comprehensive study of system performance under different
configurations. The paper presents insights into optimizing
distributed machine learning systems, enhancing their scalability and
efficiency. We also develope a framework for automating the OSMI
benchmark.</p><h2 id=introdcution>Introdcution<a class=td-heading-self-link href=#introdcution aria-label="Heading self-link"></a></h2><p>With the proliferation of machine learning as a tool for science, the
need for efficient and scalable systems is paramount. This paper
explores the Open Surrogate Model Inference (OSMI) Benchmark, a tool
for testing the performance of machine-learning systems via
machine-learned surrogate models. The OSMI Benchmark, originally
created by Wes Brewer and colleagues, serves to evaluate various
configurations and their impact on system performance.</p><p>Our research pivots around the deployment and analysis of the OSMI
Benchmark across various hardware platforms, including the
high-performance computing (HPC) system Rivanna, Windows Subsystem for
Linux (WSL), and Ubuntu environments.</p><p>In each experiment, there are a variable number of TensorFlow model
server instances, overseen by a HAProxy load balancer that distributes
inference requests among the servers. Each server instance operates on
a dedicated GPU, choosing between the V100 or A100 GPUs available on
Rivanna. This setup mirrors real-world scenarios where load balancing
is crucial for system efficiency.</p><p>On the client side, we initiate a variable number of concurrent
clients executing the OSMI benchmark to simulate different levels of
system load and analyze the corresponding inference throughput.</p><p>On top of the original OSMI-Bench, we implemented an object-oriented
interface in Python for running experiments with ease, streamlining
the process of benchmarking and analysis. The experiments rely on
custom-built images based on NVIDIA&rsquo;s tensorflow image. The code works
on several hardwares, assuming the proper images are built.</p><p>Additionally, We develop a script for launching simultaneous
experiments with permutations of pre-defined parameters with Cloudmesh
Experiment-Executor. The Experiment Executor is a tool that automates
the generation and execution of experiment variations with different
parameters. This automation is crucial for conducting tests across a
spectrum of scenarios.</p><p>Finally, we analyze the inference throughput and total time for each
experiment. By graphing and examining these results, we draw critical
insights into the performance dynamics of distributed machine learning
systems.</p><p>In summary, a comprehensive examination of the OSMI Benchmark in
diverse distributed ML systems is provided. We aim to contribute to
the optimization of these systems, by providing a framework for
finding the best performant system configuration for a given use
case. Our findings pave the way for more efficient and scalable
distributed computing environments.</p><p>The architectural view of the benchmarks are depictued in Figure 1 and
Figure 2.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:938px><img class=card-img-top src=/docs/surrogates/osmi/osmi1_hu7c0ce91db4341889bc7ce96dfb596a67_74813_928x306_fill_catmullrom_smart1_3.png width=928 height=306><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 1: Surrogate calculations via a Inference Server.</p></figcaption></figure><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:2911px><img class=card-img-top src=/docs/surrogates/osmi/osmi2_hufa595329706607e7c24e0c5ae57ccfec_402285_2901x1173_fill_catmullrom_smart1_3.png width=2901 height=1173><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Figure 2: Possible benchmark configurations to measure sped of parallel iference.</p></figcaption></figure><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Brewer, Wesley, Daniel Martinez, Mathew Boyer, Dylan Jude, Andy
Wissink, Ben Parsons, Junqi Yin, and Valentine Anantharaj. &ldquo;Production
Deployment of Machine-Learned Rotorcraft Surrogate Models on HPC.&rdquo; In
2021 IEEE/ACM Workshop on Machine Learning in High Performance
Computing Environments (MLHPC), pp. 21-32. IEEE, 2021,
<a href=https://ieeexplore.ieee.org/abstract/document/9652868>https://ieeexplore.ieee.org/abstract/document/9652868</a>, Note that
OSMI-Bench differs from SMI-Bench described in the paper only in that
the models that are used in OSMI are trained on synthetic data,
whereas the models in SMI were trained using data from proprietary CFD
simulations. Also, the OSMI medium and large models are very similar
architectures as the SMI medium and large models, but not identical.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Brewer, Wesley, Greg Behm, Alan Scheinine, Ben Parsons, Wesley Emeneker, and Robert P. Trevino. &ldquo;iBench: a distributed inference simulation and benchmark suite.&rdquo; In 2020 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1-6. IEEE, 2020.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Brewer, Wesley, Greg Behm, Alan Scheinine, Ben Parsons, Wesley Emeneker, and Robert P. Trevino. &ldquo;Inference benchmarking on HPC systems.&rdquo; In 2020 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1-9. IEEE, 2020.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Brewer, Wesley, Chris Geyer, Dardo Kleiner, and Connor Horne. &ldquo;Streaming Detection and Classification Performance of a POWER9 Edge Supercomputer.&rdquo; In 2021 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1-7. IEEE, 2021.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Gregor von Laszewski, J. P. Fleischer, and Geoffrey
C. Fox. 2022. Hybrid Reusable Computational Analytics Workflow
Management with Cloudmesh. <a href=https://doi.org/10.48550/ARXIV.2210.16941>https://doi.org/10.48550/ARXIV.2210.16941</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e825d671002d82761de7d86a532a7008>6.9 - Particle dynamics</h1><div class=lead>Recurrent Neural Nets as a Particle Dynamics Integrator</div><p>Recurrent Neural Nets as a Particle Dynamics Integrator</p><p>The second IU initial application shows a rather different type of
surrogate and illustrates an SBI goal to collect benchmarks covering a
range of surrogate designs. Molecular dynamics simulations rely on
numerical integrators such as Verlet to solve Newton&rsquo;s equations of
motion. Using a sufficiently small time step to avoid discretization
errors, Verlet integrators generate a trajectory of particle positions
as solutions to the equations of motions. In <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup>,</sup> <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup><sup>,</sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, the IU team
introduces an integrator based on recurrent neural networks that is
trained on trajectories generated using the Verlet integrator and
learns to propagate the dynamics of particles with timestep up to 4000
times larger compared to the Verlet timestep. As shown in Fig. 4
(right) the error does not increase as one evolves the system for the
surrogate while standard Verlet integration in Fig. 4 (left) has
unacceptable errors even for time steps of just 10 times that used in
an accurate simulation. The surrogate demonstrates a significant net
speedup over Verlet of up to 32000 for few-particle (1 - 16) 3D
systems and over a variety of force fields including the Lennard-Jones
(LJ) potential. This application uses a recurrent plus dense neural
network architecture and illustrates an important approach to learning
evolution operators which can be applied across a variety of fields
including Earthquake science (IU work in progress) and Fusion <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:946px><img class=card-img-top src=/docs/surrogates/particle-dynamics/particle_hue54285a9210a676902d6eb02cd7acc5c_494205_936x240_fill_catmullrom_smart1_3.png width=936 height=240><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Fig. 4: Average error in position updates for 16 particles interacting
with an LJ potential, The left figure is standard MD with error
increasing for ∆t as 10, 40, or 100 times robust choice (0.001). On
the right is the LSTM network with modest error up to t = 106 even for
∆t = 4000 times the robust MD choice.</p></figcaption></figure><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>JCS Kadupitiya, Geoffrey C. Fox, Vikram Jadhao, “GitHub
repository for Simulating Molecular Dynamics with Large
Timesteps using Recurrent Neural Networks.”
[Online]. Available:
<a href=https://github.com/softmaterialslab/RNN-MD>https://github.com/softmaterialslab/RNN-MD</a>. [Accessed: 01-May-2020]&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>J. C. S. Kadupitiya, G. C. Fox, and V. Jadhao, “Simulating
Molecular Dynamics with Large Timesteps using Recurrent Neural
Networks,” arXiv [physics.comp-ph], 12-Apr-2020
[Online]. Available: <a href=http://arxiv.org/abs/2004.06493>http://arxiv.org/abs/2004.06493</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>J. C. S. Kadupitiya, G. Fox, and V. Jadhao, “Recurrent Neural
Networks Based Integrators for Molecular Dynamics Simulations,”
in APS March Meeting 2020, 2020 [Online]. Available:
<a href=http://meetings.aps.org/Meeting/MAR20/Session/L45.2>http://meetings.aps.org/Meeting/MAR20/Session/L45.2</a>. [Accessed: 23-Feb-2020]&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>J. Kates-Harbeck, A. Svyatkovskiy, and W. Tang, “Predicting
disruptive instabilities in controlled fusion plasmas through
deep learning,” Nature, vol. 568, no. 7753, pp. 526–531,
Apr. 2019 [Online]. Available:
<a href=https://doi.org/10.1038/s41586-019-1116-4>https://doi.org/10.1038/s41586-019-1116-4</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d65c9f63f13fcf64fe06b834161e6a0d>6.10 - PtychoNN: deep learning network for ptychographic imaging that predicts sample amplitude and phase from diffraction data.</h1><div class=lead>A DL-based approach to solve the ptychography data inversion problem that learns a direct mapping from the reciprocal space data to the sample amplitude and phase.</div><hr><h2 id=metadata>Metadata<a class=td-heading-self-link href=#metadata aria-label="Heading self-link"></a></h2><hr><p><strong>Model</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/p/ptychonn.json>ptychonn.json</a></p><p><strong>Datasets</strong> <a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/datasets/p/ptychonn_20191008_39.json>ptychonn_20191008_39.json</a></p><hr><p>PtychoNN, uses a deep convolutional neural network to predict realspace structure and phase from far-field diffraction data. It recovers high fidelity amplitude and phase contrast images of a real sample hundreds of times faster
than current ptychography reconstruction packages and reduces sampling requirements <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Mathew J. Cherukara, Tao Zhou, Youssef Nashed, Pablo Enfedaque, Alex Hexemer, Ross J. Harder, Martin V. Holt; AI-enabled high-resolution scanning coherent diffraction imaging. Appl. Phys. Lett. 27 July 2020; 117 (4): 044103. <a href=https://doi.org/10.1063/5.0013065>https://doi.org/10.1063/5.0013065</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-dfd48793659645274fbc4a4e0fa01cc1>7 - Software</h1><div class=lead>Some software that we developed</div><p>A list of software we use to make things easiers</p></div><div class=td-content><h1 id=pg-8f4c333ddb68886d5b962f16d12c4b30>7.1 - cloudmesh</h1><div class=lead>cloudmesh is a flexible framework to develop cloud and HPC programs using python. It is based on a number of plugins.</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Cloudmesh allows the creation of an extensible commandline and commandshell tool based internally on a number of python APIs that can be loaded conveniently through plugins.</p><p>Plugins useful for this effort include</p><ul><li>cloudmesh-vpn<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> &ndash; a convenient way to configure VPN</li><li>cloudmesh-common<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> &ndash; useful common libraries including a StopWatch for benchmarking</li><li>cloudmesh-cmd5<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> &ndash; a plugin manager that allows plugins to be integrated as commandline tool or command shell</li><li>cloudmesh-ee<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> &ndash; A pluging to create AI grid searchs using LSF and SLURM jobs</li><li>cloudmesh-cc<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> &ndash; A plugin to allow benchmarks to be run in coordination on heterogeneous compute resources and multiple clusters</li><li>cloudmesh-apptainer<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> &ndash; mangae apptainers via a Python API</li></ul><p>Cloudmesh has over 100 plugins coordinated at <a href=http://github.com/cloudmesh>http://github.com/cloudmesh</a></p><p><sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/cloudmesh-vpn>https://github.com/cloudmesh-vpn</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://github.com/cloudmesh-common>https://github.com/cloudmesh-common</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/cloudmesh-cmd5>https://github.com/cloudmesh-cmd5</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://github.com/cloudmesh-ee>https://github.com/cloudmesh-ee</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://github.com/cloudmesh-cc>https://github.com/cloudmesh-cc</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://github.com/cloudmesh-apptainer>https://github.com/cloudmesh-apptainer</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Gregor von Laszewski, J. P. Fleischer, and Geoffrey
C. Fox. 2022. Hybrid Reusable Computational Analytics Workflow
Management with Cloudmesh. <a href=https://doi.org/10.48550/ARXIV.2210.16941>https://doi.org/10.48550/ARXIV.2210.16941</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fb12f1a23bf3d5978f884fefe1d63f2b>7.2 - sabath</h1><div class=lead>SABATH provides benchmarking infrastructure for evaluating scientific ML/AI models. It contains support for scientific machine learning surrogates from external repositories such as SciML-Bench.</div><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>SABATH provides benchmarking infrastructure for evaluating scientific ML/AI models. It contains support for scientific machine learning surrogates from external repositories such as SciML-Bench.</p><p>The software dependences are explicitly exposed in the surrogate model definition, which allows the use of advanced optimization, communication, and hardware features. For example, distributed, multi-GPU training may be enabled with Horovod. Surrogate models may be implemented using TensorFlow, PyTorch, or MXNET frameworks.</p><h2 id=models>Models<a class=td-heading-self-link href=#models aria-label="Heading self-link"></a></h2><p>Models are collected so far at</p><ul><li><p><a href=https://github.com/icl-utk-edu/sabath/tree/main/var/sabath/assets/sabath/models>https://github.com/icl-utk-edu/sabath/tree/main/var/sabath/assets/sabath/models</a></p><ul><li><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/a/autophasenn.json>autophasenn.json</a></li><li><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/c/cosmoflow.json>cosmoflow.json</a></li><li><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/m/miniWeatherML.json>miniWeatherML.json</a></li><li><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/n/nanoconfinement.json>nanoconfinement.json</a></li><li><a href=https://github.com/icl-utk-edu/sabath/blob/main/var/sabath/assets/sabath/models/p/ptychonn.json>ptychonn.json</a></li></ul></li></ul><p><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h2 id=references>References<a class=td-heading-self-link href=#references aria-label="Heading self-link"></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://github.com/icl-utk-edu/sabath>https://github.com/icl-utk-edu/sabath</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-99bc04328a000750d727f1aa33ff141a>8 - Meeting Notes</h1><div class=lead>Meeting Notes</div></div><div class=td-content><h1 id=pg-8213bcaea95358ab160234a01af4dac8>8.1 - Poster</h1><div class=lead>The SBI FAIR Poster.</div><p>We are happy to announce a poster about the SBI FAIR project.</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:938px><img class=card-img-top src=/docs/notes/poster/poster_hu19652a362abf9ac985fe695977c16e31_2079999_928x710_fill_catmullrom_smart1_3.png width=928 height=710><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>Poster <sbi-fair-poster-finalv2.pdf></p></figcaption></figure></div><div class=td-content style=page-break-before:always><h1 id=pg-71c2009b324938d14a8ce55b6942149b>8.2 - Links</h1><div class=lead>Links</div><p><strong>Overall Project Links</strong></p><ul><li><strong>Google-group:</strong> <a href=https://groups.google.com/g/sbi-fair>https://groups.google.com/g/sbi-fair</a></li><li><strong>Website:</strong> <a href=https://sbi-fair.github.io/>https://sbi-fair.github.io/</a><ul><li><strong>Publications:</strong> <a href=https://sbi-fair.github.io/docs/publications/>https://sbi-fair.github.io/docs/publications/</a></li></ul></li><li><strong>The directory from proposal writing:</strong> <a href="https://drive.google.com/drive/folders/1z1lGt8k3uqt6JGHL4mLgRSZQr9Y2oTHQ?usp=sharing">DOE_FAIR2020-Surrogates</a></li><li><strong>Directory for this proposal:</strong> <a href="https://drive.google.com/drive/folders/1lGc-hlJu7-uRk0MII-_nEXAnwWMc5uEI?usp=sharing">Afteraward</a></li><li><strong>Meeting Summaries Report:</strong>: <a href=https://docs.google.com/document/d/1cqMOkV9Cag6EB6HI6fR20gwhVwUeG5yijtJ3aEW0Crs>https://docs.google.com/document/d/1cqMOkV9Cag6EB6HI6fR20gwhVwUeG5yijtJ3aEW0Crs</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e65be94dc6e2f46c51835ce41a181d32>8.3 - Meeting Notes 02-05-2024</h1><div class=lead>Meeting Notes from 02-05-2024</div><h2 id=notes>Notes<a class=td-heading-self-link href=#notes aria-label="Heading self-link"></a></h2><ul><li><a href=https://docs.google.com/document/d/1E-eP45yqDofctCfzCb4QWiDmfYwXvXKGde6ex1c1fWg/edit>https://docs.google.com/document/d/1E-eP45yqDofctCfzCb4QWiDmfYwXvXKGde6ex1c1fWg/edit</a></li></ul><h2 id=virginia>Virginia<a class=td-heading-self-link href=#virginia aria-label="Heading self-link"></a></h2><ul><li><p>Virginia started a list of surrogates that would help prepare any poster necessary
<a href="https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_151">https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_151</a></p></li><li><p>Virginia status is <a href="https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_100">https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_100</a> and other slides here plus <a href="https://docs.google.com/presentation/d/1fqKphJlK4Q_zE1wIAxHs73c4LHFjAKzjXIiMSS_opnw/edit?usp=sharing">https://docs.google.com/presentation/d/1fqKphJlK4Q_zE1wIAxHs73c4LHFjAKzjXIiMSS_opnw/edit?usp=sharing</a></p></li><li><p>Web page <a href=https://sbi-fair.github.io/>https://sbi-fair.github.io/</a></p></li></ul><h2 id=rutgers>Rutgers<a class=td-heading-self-link href=#rutgers aria-label="Heading self-link"></a></h2><p><a href="https://docs.google.com/presentation/d/1AYvzpd4-UbAs_bRDVq4Amq9y-Z3njU3Jq5hgEXQMyEM/edit?usp=sharing">ASCR-PI-Meeting-Feb-2024-Rutgers</a></p><h2 id=indiana>Indiana<a class=td-heading-self-link href=#indiana aria-label="Heading self-link"></a></h2><ul><li>Indiana has 2 surrogates.</li><li><a href=http://localhost:1313/docs/surrogates/ions-in-nanoconfinement/>Ions in nano confinement</a>.This code allows users to simulate ions confined between material surfaces that are nanometers apart, and extract the associated ionic structure.</li></ul><p>time evolution: <a href=https://github.com/softmaterialslab/RNN-MD>GitHub</a>: Code for our paper &ldquo;Simulating Molecular Dynamics with Large Timesteps using Recurrent Neural Networks&rdquo;</p><p>See powerpoint <a href="https://docs.google.com/presentation/d/1uX1YrNGbcqmhcnT92GdzxIrKmM7qZprH/edit?usp=drive_link">sbi_Jadhao_2024.pptx</a></p><h2 id=anl>ANL<a class=td-heading-self-link href=#anl aria-label="Heading self-link"></a></h2><ul><li><a href="https://docs.google.com/presentation/d/18ytZOELRRzYTBBFnPdZX_rqWnGB3pl4n/edit?usp=drive_link">PPT SBI_Slides2024_ANL.pptx</a></li></ul><h2 id=utk>UTK<a class=td-heading-self-link href=#utk aria-label="Heading self-link"></a></h2><p><a href="https://docs.google.com/presentation/d/1SVGkNOhuC2OoAfeT-icABZvlm1zWIxO4IlwQb0_dJcw/edit?usp=sharing">SABATH Harness</a></p><h2 id=other>Other<a class=td-heading-self-link href=#other aria-label="Heading self-link"></a></h2><p>Last Joint Presentation SBI DOE Presentation November 28 <a href="https://docs.google.com/presentation/d/1d6sX3017Mz4lMqmjhAtgAKp0lgHS8f6W/edit?usp=sharing&amp;ouid=114300777188823967496&amp;rtpof=true&amp;sd=true">2022.pptx</a></p><p>The poster is FoxG_FAIR Surrogate Benchmarks .pptx or Abstract 250 words</p><ul><li><p><a href="https://docs.google.com/presentation/d/1ZrinfPiT6JALmB6n29dXfvlyj05kViPg/edit?usp=drive_link&amp;ouid=114300777188823967496&amp;rtpof=true&amp;sd=true">https://docs.google.com/presentation/d/1ZrinfPiT6JALmB6n29dXfvlyj05kViPg/edit?usp=drive_link&amp;ouid=114300777188823967496&amp;rtpof=true&amp;sd=true</a></p></li><li><p><a href="https://drive.google.com/file/d/1nsX1r_uZ6jdzB__mPWyUTqmIkvVCUpWA/view?usp=drive_link">FoxG_FAIR Surrogate Benchmarks.pdf</a></p></li></ul><p>Replacing traditional HPC computations with deep learning surrogates can dramatically improve the performance of simulations. We need to build repositories for AI models, datasets, and results that are easily used with FAIR metadata. These must cover a broad spectrum of use cases and system issues. The need for heterogeneous architectures means new software and performance issues. Further surrogate performance models are needed. The SBI (Surrogate Benchmark Initiative) collaboration between Argonne National Lab, Indiana University, Rutgers, University of Tennessee, and Virginia (lead) with MLCommons addresses these issues. The collaboration accumulates existing and generates new surrogates and hosts them (a total of around 20) in repositories. Selected surrogates become MLCommons benchmarks. The surrogates are managed by a FAIR metadata system, SABATH, developed by Tennessee and implemented for our repositories by Virginia.
The surrogate domains are Bragg coherent diffraction imaging, ptychographic imaging, Fully ionized plasma fluid model closures, molecular dynamics(2),<br>turbulence in computational fluid dynamics, cosmology, Kaggle calorimeter challenge(4), virtual tissue simulations(2), and performance tuning. Rutgers built a taxonomy using previous work and protein-ligand docking, which will be quantified using six mini-apps representing the system structure for different surrogate uses. Argonne has studied the data-loading and I/O structure for deep learning using inter-epoch and intra-batch reordering to improve data reuse. Their system addresses communication with the aggregation of small messages. They also study second-order optimizers using compression balancing accuracy and compression level. Virginia has used I/O parallelization to further improve performance. Indiana looked at ways of reducing the needed training set size for a given surrogate accuracy.</p><p>[1] Web Page for Surrogate Benchmark Initiative SBI: FAIR Surrogate Benchmarks Supporting AI and Simulation Research. Web Page, January 2024. URL: <a href=https://sbi-fair.github.io/>https://sbi-fair.github.io/</a>.
[2] E. A. Huerta, Ben Blaiszik, L. Catherine Brinson, Kristofer E. Bouchard, Daniel Diaz, Cate- rina Doglioni, Javier M. Duarte, Murali Emani, Ian Foster, Geoffrey Fox, Philip Harris, Lukas Heinrich, Shantenu Jha, Daniel S. Katz, Volodymyr Kindratenko, Christine R. Kirk- patrick, Kati Lassila-Perini, Ravi K. Madduri, Mark S. Neubauer, Fotis E. Psomopoulos, Avik Roy, Oliver R ̈ubel, Zhizhen Zhao, and Ruike Zhu. Fair for ai: An interdisciplinary and international community building perspective. Scientific Data, 10(1):487, 2023. URL: <a href=https://doi.org/10.1038/s41597-023-02298-6>https://doi.org/10.1038/s41597-023-02298-6</a>.
Note: More references can be found on the Web site</p><p>Latex version <a href=https://www.overleaf.com/project/65b7e7262188975739dae845>https://www.overleaf.com/project/65b7e7262188975739dae845</a> with PDF FoxG_FAIR Surrogate Benchmarks _abstract.pdf <a href="https://drive.google.com/file/d/1ytrrii09tKKS2AAVuUTKGw8tmM2Xf8-N/view?usp=drive_link">https://drive.google.com/file/d/1ytrrii09tKKS2AAVuUTKGw8tmM2Xf8-N/view?usp=drive_link</a></p><p>Topics</p><p>Fitting of hardware and software to surrogates
Uncertainty Quantification of the surrogate estimates
Minimize Training Data Size needed to get reliable surrogates for a given accuracy choice.
Develop and test surrogate Performance Models
Findable, Accessible, Interoperable, and Reusable FAIR data ecosystem for HPC surrogates
SBI collaborates with Industry and a leading machine learning benchmarking activity &ndash; MLPerf/MLCommons</p><p>Rutgers 2 slides
Detailed example: AI-accelerated Protein-Ligand Docking
Taxonomy and 6 mini-apps</p><p>Tennessee 6 slides
SABATH structure and UTK use
Cosmoflow in detail</p><p>Argonne 7 slides
5 slides High-Performance Data Loading Framework for Distributed DNN Training with Maximize data reuse:
Inter-Epoch Reordering (InterER) has minimal impact on the accuracy.
Intra-Batch Reordering (IntraBR) that has no impact on the accuracy.
I/O balancing
A strategy that aggregates small reads into a chunk read.</p><p>2 slides Scalable Communication Framework for Second-Order Optimizers using compression balancing accuracy and compression amount</p><p>Indiana
Goal 1: Develop surrogates for nanoscale molecular dynamics (MD) simulations
Surrogate for MD simulations of confined electrolyte ions
Surrogate for time evolution operators in MD simulations</p><p>Goal 2: Investigate surrogate accuracy dependence on training dataset size</p><p>Virginia
Work on I/O and Communicaion optimization
Done Two Argonne one IU and one MLCommons</p><p>To do
Onr argonne Fully ionized plasma fluid model closures
Calorimeter Challenge: 3 (NF:CaloFlow, Diffusion:CaloDiffusion, CaloScore v2, VAEQVAE
Last IU
UTK Cosmoflow Performance
Virtual Tissue (2)
6 Rutgers</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d149b5d4a85949257539522324849b2>8.4 - Meeting Notes 01-08-2024</h1><div class=lead>Meeting Notes from 01-08-2024</div><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Piotr Luszczek, Shantenu Jha</p><p><strong>Apologies</strong> Vikram Jadhao</p><ul><li>Shantenu described the background to the PI meeting for ASCR in February that was modeled on successful SCIDAC-wide meetings. It is not clear if sessions will be plenary or organized around Program manager portfolios.</li><li>Virginia started a list of surrogates that would help prepare any poster necessary</li><li><a href="https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_151">https://docs.google.com/presentation/d/1LonfbydMlQyLBv5vh8tjATv9BxdN7GmjuU8RFyuK5aw/edit#slide=id.g2acfd0f37ff_1_151</a></li><li>Argonne would add work on I/O, compression, and second-order methods.</li><li>Rutgers has surrogates to list, plus work on effective performance and their taxonomy of surrogate types.</li><li>Indiana was not available due to travel, but has work on data dependence and surrogates for sustainability (a new paper).</li><li>Tennessee has two surrogates, MiniWeatherML and Performance. Also has SABATH</li><li>We did not set a next meeting until the PI meeting was clearer.</li><li>Later email from DOE set the poster deadline as January 29.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-152bff6e97f1acbf5589970a9aa5a7d5>8.5 - Meeting Notes 10-30-2023</h1><div class=lead>Meeting Notes from 10-30-2023</div><p><strong>Minutes of SBI-FAIR October 30 2023, Meeting</strong></p><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Piotr Luszczek, Vikram Jadhao,** **Shantenu Jha, Margaret Lentz</p><ul><li>AI for Science report <a href=https://www.anl.gov/ai-for-science-report>AI for Science, Energy, and Security Report | Argonne National Laboratory</a></li><li><a href=https://science.osti.gov/ascr/ascac/Meetings/202309>ASCAC Advanced Scientific Comput&mldr; | U.S. DOE Office of Science(SC)</a></li><li>Hal Finkel’s (Director Research ASCR Advanced Scientific Computing talk <a href=https://science.osti.gov/-/media/ascr/ascac/pdf/meetings/202309/Finkel-ASCAC-Sept2023.pdf>ASCR Research Priorities</a> is important</li><li>Anticipated Solicitations in FY 2024<ul><li>Compared to FY 2023, expect a smaller number of larger, more-broadly-scoped solicitations driving innovation across ASCR’s research community.</li><li>In appropriate areas, ASCR will expand its strategy of solicitating longer-term projects and, in most areas, encouraging partnerships between DOE National Laboratories, academic institutions, and industry.</li><li>ASCR will continue to seek opportunities to expand the set of institutions represented in our portfolio and encourages our entire community to assist in this process by actively exploring potential collaborations with a diverse set of potential partners.</li></ul></li><li>Areas of interest include, but are not limited to:<ul><li>Applied mathematics and computer science targeting quantum computing across the full software stack.</li><li>Applied mathematics and computer science focused on key topics in AI for Science, including scientific foundation models, decision support for complex systems, privacy-preserving federated AI systems, AI for digital twins, and AI for scientific programming.</li><li>Microelectronics co-design combining innovation in materials, devices, systems, architectures, algorithms, and software (including through Microelectronics Research Centers).</li><li>Correctness for scientific computing, data reduction, new visualization and collaboration paradigms, parallel discrete-event simulation, neuromorphic computing, and advanced wireless for science.</li><li>Continued evolution of the scientific software ecosystem enabling community participation in exascale innovation, adoption of AI techniques, and accelerated research productivity.</li></ul></li><li>She noted the Executive order today, <a href=https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/>Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence | The White House</a>, and this message (trustworthiness) will be reflected in DOE programs.</li><li>Microelectronics will be a thrust</li><li>NAIRR $140M is important</li></ul><p><strong>Rutgers</strong></p><pre><code>Shantenu Jha gave a thorough presentation  


There were four items below with status given in **bold**
</code></pre><ol><li>Develop and Characterize Surrogates in the Context of NVBL Pipeline<ol><li>Published in Scientific Reports: Performance of Surrogate models without loss of accuracy (Stage 1 of NVBL Drug discovery pipeline) <strong>(Done)</strong></li></ol></li><li>Performance & taxonomy of surrogates coupled to HPC (paper in a month)
2. Survey surrogates coupled to HPC simulations (Almost complete 2023-Q3)
3. Generalized framework of surrogate performance (Ongoing 2023-Q4)
1. Optimal Decision making in the DD pipeline (published)</li><li>Tools <strong>(Ongoing)</strong>
4. Preliminary work on mini-apps under review; extend to FAIR mini-apps for surrogates taxonomy
5. Deployed on DOE leadership class machines</li><li>Interact with MLCommons** (Anticipate start in 2023/Q4)**<br>6. Benchmarks for surrogate coupled to HPC workflows</li></ol><p><strong>Indiana</strong></p><ul><li>Vikram Jadhao presented</li><li>Accuracy speed up tradeoff for molecular dynamics surrogates</li><li>Looking for datasets with errors</li><li>Followed up with later discussions with Rutgers so can feed into software</li></ul><p><strong>Tennessee</strong></p><ul><li>Piotr Luszczek gave presentation</li><li>He reported on progress with SAbath and MiniWeatherML</li><li>He is giving several presentations</li></ul><p><strong>Virginia</strong></p><ul><li>**Presentation **</li><li>We discussed progress with surrogates and enhancements to Sabath</li><li>We discussed repository and noted that different models need different specific environments<ul><li>Requirements.txt will specify this</li><li>Different target hardware needs to be supported</li></ul></li><li>OSMIBench will be released before end of year</li><li>Support separate repositories in the future</li><li>We discussed papers and, in particular, a poster at the Oak Ridge OLCF users meeting.</li></ul><p><strong>Argonne</strong></p><ul><li>Finished the contract but will, of course, complete their papers.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e6023118fbee3e0ec2353782edd96ca9>8.6 - Meeting Notes 09-25-2023</h1><div class=lead>Meeting Notes from 09-25-2023</div><p><strong>Minutes of SBI-FAIR September 25 2023, Meeting</strong></p><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Piotr Luszczek, Vikram Jadhao</p><p><strong>Apologies:</strong> Shantenu Jha, Kamil Iskra, Margaret Lentz</p><p><strong>Virginia</strong></p><ul><li>**Presentation **</li><li>Repository</li><li>Specific environments are needed for different models</li><li>Requirements.txt</li><li>Different hardware support</li><li>Copy MLCommons approach</li><li>MLCube as a target</li><li>Tools to generate targets</li><li>Release before supercomputing</li><li>Add MLCommons benchmarks</li><li>Separate repositories in version 2</li></ul><p><strong>Argonne</strong></p><ul><li>Finished the contract but will, of course, complete their papers.</li></ul><p><strong>Tennessee</strong></p><ul><li>Piotr presented</li><li>SABATH updates</li><li>IBM-NASA Foundation model has multi-part datasets</li><li>Cloudmesh uses SABATH</li><li>Smokey Mountain presentation tomorrow</li></ul><p><strong>Rutgers</strong></p><ul><li>See end of</li><li>The first mini-app is ready</li></ul><p><strong>Indiana</strong></p><ul><li>Will update the nanoconfinement app and Nanohub version still used</li><li>Second surrogate being worked on</li><li>Soft label work continuing</li><li>Interested in AI for Instruments</li><li>Surrogates help Sustainability as save energy</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-48d76162e8cb20ff3e36438d8bc97914>8.7 - Meeting Notes 08-25-2023</h1><div class=lead>Meeting Notes from 08-25-2023</div><p><strong>Minutes of SBI-FAIR August 28 2023, Meeting</strong></p><ul><li>Monday, September 25, 2023, <a href=https://virginia.zoom.us/my/gc.fox>https://virginia.zoom.us/my/gc.fox</a></li></ul><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Kamil Iskra,, Baixi Sun. Piotr Luszczek,</p><p><strong>Apologies:</strong> Shantenu Jha, Vikram Jadhao, Margaret Lentz (Rutgers and Indiana not presented)</p><p><strong>Virginia</strong></p><ul><li></li><li>SABATH extensions</li><li>OSMIBench improved</li><li>Experiment Executor added in Cloudmesh</li><li>Argonne surrogates supported</li></ul><p><strong>Argonne</strong></p><ul><li>Baixi presented their new work</li><li>SOLAR paper with artifacts submitted</li><li>The communication bottleneck in the second order method K-FAC addressed with compression and sparsification methods with SSO Framework</li></ul><p><strong>Tennessee</strong></p><ul><li>Piotr described Virginia&rsquo;s enhancements</li><li>IBM-NASA multi-part datasets in Foundation model</li><li>Smokey Mountain Conference</li><li>Integration with MLCommons Croissant using Schema.org</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1824fe440fafb28768087ce57fe99e41>8.8 - Meeting Notes 07-31-2023</h1><div class=lead>Meeting Notes from 07-31-2023</div><p><strong>Minutes of SBI-FAIR July 31 2023, Meeting</strong></p><ul><li>Monday, August 28, 2023, <a href=https://virginia.zoom.us/my/gc.fox>https://virginia.zoom.us/my/gc.fox</a></li><li>Monday, September 25, 2023, <a href=https://virginia.zoom.us/my/gc.fox>https://virginia.zoom.us/my/gc.fox</a></li></ul><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Kamil Iskra, Xiaodong Yu, Baixi Sun. Piotr Luszczek, Shantenu Jha</p><p><strong>Apologies:</strong> Vikram Jadhao,</p><p><strong>Virginia</strong></p><ul><li>Geoffrey presented the Virginia Update <a href="https://docs.google.com/presentation/d/132erkV49Lgd0ZFx-AtNWJPRwTrxc480m-rU6jmvMmYA/edit?usp=sharing">https://docs.google.com/presentation/d/132erkV49Lgd0ZFx-AtNWJPRwTrxc480m-rU6jmvMmYA/edit?usp=sharing</a>, which also included Indiana (see below)</li><li>Good progress with Argonne Surrogates<ul><li>We have added PtychoNN to SABATH, and we have run AutoPhaseNN on Rivanna</li></ul></li><li>We reviewed other surrogates from Virginia including OSMIBench and a new Calorimeter simulation</li><li>We are working well with Tennessee on SABATH</li><li>Gregor finished with a little study on use of Rivanna &ndash; the Virginia Supercomputer</li></ul><p><strong>Indiana</strong></p><ul><li>Vikram is still traveling in India and was not able to join today&rsquo;s meeting. He shared by email the following updates.These are included in Virginia Presentation</li><li>the nanoconfinement surrogate repository is updated with the latest results from the sample size study published in JCTC, <a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c01282>Probing Accuracy-Speedup Tradeoff in Machine Learning Surrogates for Molecular Dynamics Simulations | Journal of Chemical Theory and Computation</a><ul><li><a href=https://github.com/softmaterialslab/nanoconfinement-md/tree/master/python/surrogate_samplesize>https://github.com/softmaterialslab/nanoconfinement-md/tree/master/python/surrogate_samplesize</a></li><li>Coordinate with Fanbo Sun who is leading the development of this surrogate and conducted the sample size study that I have shared in our meetings.</li></ul></li><li>Working on preparing the dataset for the follow-up study to JCTC</li><li>literature review: if folks are interested, the special issue on machine learning for molecular simulation in JCTC has many interesting papers (including surrogates): <a href=https://pubs.acs.org/toc/jctcce/19/14>Journal of Chemical Theory and Computation | Vol 19, No 14</a></li></ul><p><strong>Argonne</strong></p><ul><li>Argonne’s funds have essentially finished</li><li>Xiaodong Yu is moving to Stevens</li><li>New compression study comparing methods that are error bounded or nott &ndash; their performance differs by a factor of 4-6</li><li>Baixi gave an update presentation SSO: A Highly Scalable Second-order Optimization Framework ffor Deep Neural Networks via Communication Reduction</li><li>Quantized Stochastic Gradient Descent QSGD Non error bounded</li><li>Model accuracy versus compression tradeoff</li><li>Unable to utilize error-feedback due to GPU memory being filled by large models and large batch size.</li><li>Looked at different rounding methods<ul><li>Stochastic rounding preserves direction better as not so many zeros</li></ul></li><li>Revised our I/O paper i.e., SOLAR based on the reviews, submitting to ppopp’24 with new experiments and better writeup</li></ul><p><strong>Rutgers</strong></p><ul><li>The surrogate survey paper is making good progress with DeepDriveMD other motifs.</li><li>Andre Merzy is working on associated Miniapps (surrogates)</li><li>Will work with MLCommons in October</li></ul><p><strong>Tennessee</strong></p><ul><li>Piotr presented his groups work <a href="https://drive.google.com/file/d/1ep9zxdv25I3MJmPt5YcJi32SHu5BAF4J/view?usp=sharing">https://drive.google.com/file/d/1ep9zxdv25I3MJmPt5YcJi32SHu5BAF4J/view?usp=sharing</a></li><li>MiniWeatherML running with MPI and with or without CUDA.<ul><li>No external dataset is required</li></ul></li><li>SABATH making good progress in collaboration with Virginia</li><li>They are working on Cosmoflow</li><li>Piotr noted that those sites that are continuing with the project will need to submit a project report very soon. Geoffrey shared his project report to allow a common story</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-728a054cd992007e0a960e3857134b26>8.9 - Meeting Notes 06-26-2023</h1><div class=lead>Meeting Notes from 06-26-2023</div><p><strong>Minutes of SBI-FAIR June 26, 2023, Meeting</strong></p><p>**Present: **Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Kamil Iskra, Xiaodong Yu, Baixi Sun. Vikram Jadhao, Piotr Luszczek, Shantenu Jha, Margaret Lentz</p><p><strong>Virginia</strong></p><ul><li>This was presented by Geoffrey</li><li>He described work on new surrogates, including LHC Calorimeter, Epidemiology, Extended virtual tissue, and Earthquake</li><li>He described work on the repository and SABATH</li><li>This involved two existing AI models CloudMask and OSMIBench</li><li>Shantenu Jha asked about the number of inferences per second.<ul><li>From MLCommons Science Working minutes, we find for OSMIBench</li><li>On Summit, with 6 GPUs per node, one uses 6 instances of TensorFlow server per node. One uses batch sizes like 250K with a goal of a billion inferences per second</li></ul></li></ul><p><strong>Argonne</strong></p><ul><li>Continue to work on Second-order Optimization Framework for Deep Neural Networks with Communication Reduction</li><li>Baixi Sun presented the details</li><li>He introduced quantization to lower precision QSGD which gives encouraging results, although In one case quantization method failed in the eigenvalue stage</li><li>We removed Rick Stevens from the Google Group</li><li>Geoffrey mentioned his ongoing work on improving shuffling using Arrow vector format; he will share the paper when ready</li></ul><p><strong>Indiana</strong></p><ul><li>Vikram gave the presentation without slides</li><li>Continuing study of needed training set for the Ions confinement surrogate <a href=https://pubs.acs.org/doi/abs/10.1021/acs.jctc.2c01282>Probing Accuracy-Speedup Tradeoff in Machine Learning Surrogates for Molecular Dynamics Simulations | Journal of Chemical Theory and Computation</a></li><li>New dataset to explore soft labels reflecting computational uncertainty to reduce errors</li></ul><p><strong>Rutgers</strong></p><ul><li>Shantenu presented</li><li>Nice paper on surrogate classes with Wes Brewer, who works with Geoffrey on OSMIBench</li><li>Mini-apps for each of the 6 motifs that need FAIR metadata<ul><li>5 motifs use surrogates; one generates them</li></ul></li><li>He described an interesting workshop on molecular simulations</li><li>He noted that Aurora training trillion parameter foundation model for science</li><li>LLMs need 10 power 8 exaflops: Need to optimize!</li><li>Vikram noted <a href=https://arxiv.org/pdf/2112.03235.pdf>SIMULATION INTELLIGENCE: TOWARDS A NEW GENERATION OF SCIENTIFIC METHODS</a></li></ul><p><strong>Tennessee</strong></p><ul><li>Piotr presented slides</li><li>CosmoFlow on 8 GPUs is running well</li><li>He introduced the MiniWeatherML mini-app<ul><li>CUDA-aware pointers must be explicitly specified in the FAIR schema</li><li>Test in PETSc leaves threaded MPI in an invalid state</li><li>Alternative MPIX query interface varies between MPI implementations</li><li>GPU Direct copy support is optional</li></ul></li><li>SABATH system is moving ahead with a focus on adding MPI support</li><li>Piotr is now the PI of this project at UTK. We removed Cade Brown, Jack Dongarra, and Deborah Penchof from the Google Group</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0c727b9cc18c28c4dcea8e387a96729c>8.10 - Meeting Notes 05-29-2023</h1><div class=lead>Meeting Notes from 05-29-2023</div><p><strong>Minutes of SBI-FAIR May 29, 2023, Meeting</strong></p><p><strong>Present:</strong> Geoffrey Fox, Xiaodong Yu, Baixi Sun. Piotr Luszczek,</p><p><strong>Virginia</strong></p><ul><li>Comment on Surrogates produced by generative methods versus those that map particular inputs to particular outputs. In examples like experimental physics apparatus simulations, you only need output and not input. Methods need to sample output data space correctly.</li><li>Geoffrey also described earlier experiences using second-order methods and least squares/maximum likelihood optimizations for physics data analysis. One can use eigenvalue/vector decomposition or the Levenberg-Marquardt method.</li></ul><p><strong>Tennessee</strong></p><ul><li>SABATH student continuing over summer</li><li>New surrogate MiniWeatherML is not PyTorch Tensorflow from Oak Ridge</li><li>“Hello World” for weather. <a href=https://github.com/mrnorman/miniWeatherML>https://github.com/mrnorman/miniWeatherML</a></li></ul><p><strong>Argonne</strong></p><ul><li>Xiaodong summarized the situation, and Baixi gave a detailed presentation</li><li>Working on reducing data size, but compression technology seems difficult</li><li>The error-bounded approach doesn’t seem to work very well, and so Argonne are investigating other methods. There is currently no method that preserves good accuracy and gives significant reduction.</li><li>Looking at the performance of first and second-order gradients</li><li>What can you drop in second order method &ndash; lots of data are irrelevant but not what current lossy compression seems to be doing</li><li>Model parallelism for calculating eigensystems and then Data parallelism</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1193689e8ba9961b6135bc169af74a93>8.11 - Meeting Notes 04-03-2023</h1><div class=lead>Meeting Notes from 04-03-2023</div><p><strong>Minutes of SBI-FAIR April 3 2023, Meeting</strong></p><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Piotr Luszczek, Kamil Iskra, Xiaodong Yu, Baixi Sun. Vikram Jadhao, Margaret Lentz (DOE),</p><p><strong>Regrets:</strong> Shantenu Jha</p><p>**DOE **had no major announcements but reminded us of links</p><ul><li><a href=https://public.govdelivery.com/accounts/USDOEOS/subscriber/new>US Department of Energy Office of Science</a></li><li><a href=https://science.osti.gov/ascr/Funding-Opportunities>ASCR Funding Opportunities | U.S. DOE Office of Science (SC)</a></li><li>Also noted that No Cost extensions were needed six months before the official end due to the heavy load at the DOE Chicago office.</li><li>Margaret noted that University responses to Earthshot should read the laboratory call as well.</li></ul><p><strong>Virginia</strong> Geoffrey summarized activities (Slides 1-5) with a new Virtual Tissue surrogate using UNet and periodic boundary conditions. We are investigating new ideas that can describe functions with a wide dynamic range. Virginia is responsible for final deployed surrogates and building a team with Undergraduates, Researchers, and Ph.D. students. Students find experience educational, as we discovered in a collaboration with New York University. Przemek Porebski is joining the Virginia team with experience in computational epidemiology, and software engineering. Przemek introduced himself. Virginia also covered the status of MLCommons benchmarks, including new ones OSMIBench and FastML.</p><p><strong>Rutgers</strong> Shantenu was unable to attend but prepared slides and briefed them to Geoffrey, who presented them to him (Slides 6-10). These summarize the current status with a list of the six classes of surrogate problems identified as important. Shantenu compared the training samples for surrogates with that found for LLM’s. He proposes to develop mini-apps (benchmarks) covering the range of key features exhibited by surrogates.</p><p>Vikram gave <strong>Indiana University</strong>’s Presentation with a careful analysis of accuracy as a function of</p><ul><li>Dataset size showing error plateaus at acceptable values at a sample size of around 2000.</li><li>The boundary versus internal points</li><li>Sensitivity to removing selected features and how many removed points were needed for acceptable answers. Here result depended on the particular feature and measured generalizability of the network.</li><li>There is a publication under review.</li></ul><p><strong>Argonne</strong>’s new results were described by Baixi where the team was busy preparing a paper for SC23.</p><ul><li>They continued the study of second-order methods showing a broadcast was time-consuming, taking 48% of the time on 64 GPUs.</li><li>The message sizes were not large and in a region where latency was important.</li><li>They used lossy compression and studied the outliers in this.</li><li>Note the last meeting’s presentation introducing the K-FAC method.</li></ul><p>Piotr described <strong>Tennessee</strong>’s work with</p><ul><li>Focus on SABATH tested on three applications. It is nearly ready to be used by Virginia</li><li>They have identified a new graduate student and need to modify the contract where Margaret gave key advice.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-edb1615b81c946ffc8c736bb64715a13>8.12 - Meeting Notes 02-27-2023</h1><div class=lead>Meeting Notes from 02-27-2023</div><p><strong>Minutes of SBI-FAIR February 27 2023, Meeting</strong></p><p>**Present: **Geoffrey Fox, Piotr Luszczek, Gregor von Laszewski, Kamil Iskra, Xiaodong Yu, Baixi Sun. Vikram Jadhao,</p><p>We discussed modifying our simple summary describing the status and plans for the project to add a discussion of the timeline. Virginia did theirs as an example on slide 2.</p><p><strong>Indiana</strong></p><p>Vikram discussed recent activity, responding to referee comments on their recent paper.</p><p><strong>Virginia</strong></p><p>Geoffrey noted two new surrogates: A diffusion surrogate <a href=https://arxiv.org/abs/2302.03786>https://arxiv.org/abs/2302.03786</a> with James Glazier and J. Quetzalcoatl Toledo-Marin; a computational fluid dynamics surrogate <a href=https://code.ornl.gov/whb/osmi-bench>https://code.ornl.gov/whb/osmi-bench</a> from Oak Ridge</p><p>Geoffrey described issues arising from the diffusion surrogate above. We are trying to understand how deep learning can work for problems with a large range of input or output values. Examples could be covid, flu counts, images with a wide range of illumination, finding surrogate solutions where function values often range over several orders of magnitude, and one is interested in both large and small values. This range of values is seen over spatial values (images) or time values (time series)</p><p>However, this doesn&rsquo;t seem to work properly in deep learning, where the activation value is 1. The weights cannot adjust to different sizes of input values, so one cannot see the nonlinearity of activation in values over the full range. Naively the DL will choose weights, so activation nonlinearity only really impacts a portion of the value range. One can think of many approaches</p><p>a) replace value by value**n for n &lt; 1 including log value</p><p>b) scale activation value by an average value (found from a coarser scale if labeled by space as in an image)</p><p>c) Mixture of experts with different values of activation for each expert such as 0.001 0.01 0.1 1</p><p><strong>Tennessee</strong></p><p>Piotr reported that the SABATH project had a new student and was ramping up.</p><p><strong>Argonne</strong></p><p>Baixi discussed second-order optimization using Kronecker-factored Approximate Curvature K-FAC, which significantly outperforms standard Stochastic Gradient Descent. This is coupled with compression to reduce communication costs.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9702267f6758080ea10e254acb342cdf>8.13 - Meeting Notes 01-30-2023</h1><div class=lead>Meeting Notes from 01-30-2023</div><p><strong>Minutes of SBI-FAIR January 2, 9, and 30 2023, Meetings</strong></p><p><strong>January 2 2023:</strong></p><p>**Present: **Deborah Penchoff, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Gregor von Laszewski</p><p>We discussed producing a simple summary (roughly one slide per institution) describing the status and plans for the project. Virginia, UTK, and Rutgers made a draft which will be expanded before our January 30 meeting with Margaret. These should mention inter-institution collaborations. We continued on January 9</p><p><strong>January 9 2023:</strong></p><p>**Present: **Geoffrey Fox, Kamil Iskra, Xiaodong Yu, Baixi Sun. Vikram Jadhao, Gregor von Laszewski</p><p>Based on the earlier meeting, Argonne and Indiana produced summary pages which we iterated to include collaborations to deposit surrogates in the repository.</p><p><strong>January 30, 2023:</strong></p><p><strong>Present:</strong> not recorded, but all institutions represented</p><p>We gave our presentation and followed with a discussion with Margaret. She noted recent DOE calls with useful links</p><pre><code>https://public.govdelivery.com/accounts/USDOEOS/subscriber/new

https://science.osti.gov/ascr/Funding-Opportunities
</code></pre><p>She stressed the importance of establishing a timeline. We should discuss at the next meeting.</p><p>We didn’t decide on a cadence for her presence at our meetings.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ad039a91808ad7bbe0578a4521b0524f>8.14 - Meeting Notes 01-05-2023</h1><div class=lead>Meeting Notes from 01-05-2023</div><p><strong>Minutes of SBI-FAIR May 1, 2023, Meeting</strong></p><p><strong>Present:</strong> Geoffrey Fox, Gregor von Laszewski, Przemek Porebski, Kamil Iskra, Xiaodong Yu, Baixi Sun. Vikram Jadhao, Piotr Luszczek,</p><p><strong>Regrets:</strong> Shantenu Jha</p><p><strong>Virginia</strong> Geoffrey noted continued progress with the new Virtual Tissue surrogate using UNet and periodic boundary conditions. Interesting that UNet mimics multigrid PDE methods. Przemyslaw still disentangling from other work but will start very soon. Several (50 in 2 weeks) undergraduate and incoming graduate student research requests. Surrogate OSMIBench progress and will integrate with SABATH. Geoffrey asked what surrogates are available to work on now.</p><p><strong>Rutgers</strong></p><p>Not presented</p><p><strong>Indiana University</strong></p><p>Vikram discussed progress. Ions in confinement code will be sent to UVA. Discussed sensitivity to training data showing the need for some but not all samples in a region.</p><p><a href=https://pubs.acs.org/doi/10.1021/acs.jctc.2c01282>https://pubs.acs.org/doi/10.1021/acs.jctc.2c01282</a> and PDF is</p><p>Studied interpolation; extend to extrapolation</p><p>Speedup study &ndash; the factor of 2 if one drops every other point and replace them by a small fraction of these interpolations</p><p><strong>Argonne</strong></p><p>The SOLAR paper was rejected.</p><p>Baixi presented their new results with a focus on data compression (for second-order optimization)</p><p>Aggregate Broadcast as previously latency dominated</p><p>Float32 versus Float64 inversion error (eigensolution versus inversion)</p><p>Some tasks are sensitive to precision.</p><p>Submitted to SC23; will share with people</p><p>Communicated Light Source Surrogates PtychoNN and AutoPhaseNN to the FAIR main repository. Baixi asked Dr. Cherukara (from ANL) and got permission about which can be available to the public.</p><ul><li>Currently, PtychoNN has the Code, trained model weights, training, and test data on GitHub: <a href=https://github.com/mcherukara/PtychoNN>https://github.com/mcherukara/PtychoNN</a>.</li><li>AutophaseNN has Code, trained models, and test data available on GitHub: <a href=https://github.com/YudongYao/AutoPhaseNN>https://github.com/YudongYao/AutoPhaseNN</a>.</li></ul><p>Specifically, they implemented PytchoNN using PyTorch Distributed Data-Parallel (DDP)</p><p>See Onedrive <a href="https://indiana-my.sharepoint.com/:f:/g/personal/sunbaix_iu_edu/EsKMnXradjpCkaKFSfcdQlMBp7BEG7gIiTNifbutL_RUVw?e=zYhhnh">FAIR</a> Or please use this google drive link:</p><p><a href="https://drive.google.com/drive/folders/1c2HGFBiymJUu9yaUTW5K-dIOoemxOfjN?usp=sharing">https://drive.google.com/drive/folders/1c2HGFBiymJUu9yaUTW5K-dIOoemxOfjN?usp=sharing</a><span style=text-decoration:underline> </span>These have the same readme and Python files</p><p><strong>Tennessee</strong></p><p>Piotr presented CUDA 10 versus CUDA 11</p><p>SABATH Cosmoflow small dataset working. Move to</p><ul><li>Earthquake</li><li>OSMIBench</li></ul><p>Gregor described progress with Friday May 14 1 pm meeting with Wes Brewer</p><p>Gregor recommends exchanging Docker or Singularity definition files</p><p>SABATH could create the container image</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6926e251b710e799f3ba9380666cb126>8.15 - Meeting Notes 11-28-2022</h1><div class=lead>Meeting Notes from 11-28-2022</div><p><strong>Minutes of SBI-FAIR November 28, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Xiaodong Yu, Deborah Penchoff, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Baixi Sun. Vikram Jadhao, Gregor von Laszewski and Margaret Lentz from DOE</p><p>Preparations/drafts: <a href="https://drive.google.com/drive/folders/1dRKDJgN9yT8OVwT1PUyYYTsnjmhCep9h?usp=sharing">Nov 28 2022 DOE Project Review Preparations</a></p><p>Actually delivered presentations are has on the first slide links to individual presentations in the order</p><ul><li>Virginia</li><li>Tennessee</li><li>Argonne</li><li>Rutgers</li><li>Indiana</li></ul><p>Margaret emphasized the need for continued interaction and we scheduled the next meeting with Margaret on January 30, 2023.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9e87d0c6e4d002550de608a5cf492c85>8.16 - Meeting Notes 10-31-2022</h1><div class=lead>Meeting Notes from 10-31-2022</div><p><strong>Minutes of SBI-FAIR October 31, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Xiaodong Yu, Peter Beckman, Deborah Penchoff, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Baixi Sun. Vikram Jadhao, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><p>Geoffrey discussed</p><ul><li>The transfer of the DOE grant is completed</li><li>The Tsunami surrogate (see last meeting) is finished while the diffusion-based surrogate is still being finalized<ul><li>Rough draft of the diffusion model for cell simulations GENERALIZATION AND TRANSFER LEARNING IN A DEEP DIFFUSION SURROGATE FOR MECHANISTIC REAL-WORLD SIMULATIONS. Interesting is the study of dataset sizes 5000-400,000 and the importance of dealing with the large numeric range in computed values</li></ul></li><li>We discussed Margaret Lentz’s request for a project presentation<ul><li>Draft after SC22 with final presentation November 28 1-2 pm finalized with Margaret</li><li>Some integrating slides and then 4-6 from each team covering past work; remaining work in the grant; what to do after the grant</li><li>Pete reminded us not to forget FAIR!</li><li>Geoffrey will make a plan</li></ul></li></ul><p><strong>Argonne</strong></p><ul><li>Their VLDB2023 paper: “SOLAR: A Highly Optimized Data Loading Framework Training CNN-based Scientific Surrogates,” was discussed</li><li>This paper looks at the training of 3 surrogates and addresses the overhead of the I/O disk access that dominates the performance</li><li>They compare with PyTorch Data Loader and the NoPFS paper <a href=https://arxiv.org/abs/2101.08734>[2101.08734] Clairvoyant Prefetching for Distributed Machine Learning I/O</a> from Torsten Hoefler at the last SC meeting. This does optimized prefetching</li><li>The shuffle is optimized to minimize redistribution and this leads to an improvement factor of 3.5 over NoPFS and 24 over default PyTorch \</li></ul><p><strong>Tennessee</strong></p><p>Piotr reported that Cade Brown has left and they are hiring a replacement.</p><p><strong>Rutgers</strong></p><p>Shantenu reported</p><ul><li>That their team had identified 6 categories with AI enhancing HPC and they were studying performance</li><li>He returned to topic of Large Language models LLM that can be effective in chemistry,</li></ul><p><strong>Indiana University</strong></p><p>Vikram reported that</p><ul><li>They were continuing study of accuracy and robustness as last time as well as</li><li>Dataset size</li><li>Ensemble issues</li><li>Definition of speedup</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3829bc23ea44f58891cd7a2ddea19ffb>8.17 - Meeting Notes 09-26-2022</h1><div class=lead>Meeting Notes from 09-26-2022</div><p><strong>Minutes of SBI-FAIR September 26, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Xiaodong Yu, Deborah Penchoff, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Baixi Sun. Vikram Jadhao, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><p>Geoffrey discussed</p><ul><li>The transfer of the DOE grant is still making progress</li><li>He noted two nearly completed new surrogates<ul><li>paper on Tsunami simulation surrogates entitled “Forecasting tsunami inundation with convolutional neural networks for a potential Cascadia Subduction Zone rupture”</li><li>Rough draft of the diffusion model for cell simulations GENERALIZATION AND TRANSFER LEARNING IN A DEEP DIFFUSION SURROGATE FOR MECHANISTIC REAL-WORLD SIMULATIONS. Interesting is the study of dataset sizes 5000-400,000 and the importance of dealing with the large numeric range in computed values</li></ul></li><li>He summarized the MLCommons status with the move to continuous (rolling) submissions rather than fixed date submissions</li></ul><p><strong>Indiana University</strong></p><ul><li>Vikram presented some of his recent work</li><li>He studied sensitivity to input training set showing some dramatic effects from seemingly small changes &ndash; removing one value of electrolyte concentration c</li></ul><p><strong>Tennessee</strong></p><p>Piotr reported</p><ul><li>There was a Data Challenge at Smoky Mountain meeting with a smaller version of the Cloudmask dataset from MLCommons <a href=https://smc-datachallenge.ornl.gov/ch6-satellite-datasets/>2022 Challenge 6: SMCEFR: Sentinel-3 Satellite Dataset « SMC Data Challange 2021</a></li><li>Two Submitted papers: one on Performance Surrogate and the other a SABATH paper at HPEC Conference <a href=https://ieee-hpec.org/>IEEE HPEC</a> 26th Annual 2022 IEEE High Performance Extreme Computing Virtual Conference 19 - 23 September 2022<ul><li>paper and presentation Deep Gaussian process with multitask and transfer learning for performance optimization</li></ul></li><li>Questions included reproducibility and overheads from using FAIR metadata</li><li>It was asked if SABATH recorded training time; it does record loss versus epoch number.</li><li>Tennessee will give a detailed presentation on SABATH next time.</li></ul><p><strong>Rutgers</strong></p><p>Shantenu reported</p><ul><li>Drug and Quantum surrogates</li><li>He noted a new DOE $25M award for climate surrogates revisiting the startling Oxford paper <a href=https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffa/meta>https://iopscience.iop.org/article/10.1088/2632-2153/ac3ffa/meta</a> and <a href=https://arxiv.org/pdf/2001.08055v1>https://arxiv.org/pdf/2001.08055v1</a></li><li>Work with Indiana University was continuing with efforts to get system running on Summit</li><li>There was a discussion of Large Language models LLM and DOE interest in using them on scientific literature. There is a challenge with the current $10-100 million computing training cost possibly reaching a billion dollars.</li></ul><p><strong>Argonne</strong></p><ul><li>Xiaodong Yu discussed the ASPLOS paper which was unfortunately rejected</li><li>Baixi presented their results commenting on referee remarks</li><li>One question prompted observation that surrogate MODEL sizes are comparatively small</li><li>Another question was answered by noting that scheduling was a one-time cost</li><li>In some cases their custom training order outperformed the baseline training</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1cd978086b98f4fae66b30ea114be812>8.18 - Meeting Notes 08-15-2022</h1><div class=lead>Meeting Notes from 08-15-2022</div><p><strong>Minutes of SBI-FAIR August 15, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Xiaodong Yu, Deborah Penchoff, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Baixi Sun.</p><p><strong>Apologies</strong> Vikram Jadhao,</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><p>Geoffrey discussed</p><ul><li>The transfer of the DOE grant is making progress</li><li>He is continuing his study of Foundation models by collecting common applications using similar deep learning systems</li><li>He summarized the MLCommons status answering some questions noting that MLCommons collects surrogates and non-surrogate benchmarks<ul><li>Geoffrey will send Shantenu notice about MLCommons meetings</li></ul></li></ul><p>Gregor</p><ul><li>contacted Rutgers for help, but due to staff changes that effort was shifted to Summit support team. Activity in progress.</li></ul><p><strong>Rutgers</strong></p><p>Shantenu reported</p><ul><li>Work with Indiana University was delayed as JCS Kadupitiya has graduated from IU and was hired by Microsoft</li><li>Improving AI for Science Chapter with AI-linked workflows for a new publication with performance</li></ul><p><strong>Argonne</strong></p><ul><li>Xiaodong Yu discussed the ASPLOS paper and will send an improved version in 2 weeks</li><li>There are performance issues addressed with microbenchmarks</li><li>Baixi presented their results optimized over epoch and batch</li><li>This does not change results much even though the update order is different</li><li>Schedule by access performance or load balance</li><li>4.2 to 5.8 speedup up to 64 processes</li><li>Looking at scalability</li><li>Other surrogates are AutophaseNN and BraggNN</li></ul><p><strong>Indiana University</strong></p><p>Reported by email</p><ul><li>Starting Fall 2022, a new PhD student Fanbo Sun and a new postdoc Wenhui Li will work 50% on this project. Postdoc starts Sep 1.</li><li>Soft labels: Continuing to explore the soft labels idea and how it reduces training set sizes. Planning a submission sometime this year. One paper submitted last year on this topic is still under review.</li><li>Time series surrogate: With the postdoc, we will be working to extend the RNN operator to tackle NVT ensemble and larger number of particles.</li></ul><p><strong>Tennessee</strong></p><p>Piotr reported</p><ul><li>Cade will come back plus a new Ph.D. student</li><li>Two Submitted papers: one on Performance Surrogate and the other a SABATH paper</li><li>Third paper to Data Challenge</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f3c338aaa79ded581231c48cbf59f301>8.19 - Meeting Notes 06-27-2022</h1><div class=lead>Meeting Notes from 06-27-2022</div><p><strong>Minutes of SBI-FAIR June 27, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Deborah Penchoff, Vikram Jadhao, Shantenu Jha, Geoffrey Fox, Piotr Luszczek, Baixi Sun, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><ul><li>Foundation Models &ndash; collect surrogates</li><li>Need a group DOE report. Use last year&rsquo;s approach with a common response initialized by Piotr <a href="https://docs.google.com/document/d/19cbBj2IMIMa_HUPAUaREy3zRYBQfEoS5hCmM7wyU48c/edit?usp=sharing">https://docs.google.com/document/d/19cbBj2IMIMa_HUPAUaREy3zRYBQfEoS5hCmM7wyU48c/edit?usp=sharing</a></li><li>Note plans went a bit different as due to transfer Indiana and Virginia were not asked for annual reports</li></ul><p><strong>Tennessee</strong></p><ul><li>SABATH software</li><li>MLCommons Paper ISC Piotr Luszczek went and did not get Covid. BOF presentation from Piotr and H3 conference <a href=http://www.icl.utk.edu/~luszczek/conf/2022/h3/>H3 workshop</a> report from Jeyan Thiyagalingam.</li><li></li></ul><p><strong>Rutgers</strong></p><ul><li>Vincent Pascuzzi has a prototype software system running with JCS Kadupitiya</li><li>Davis DOE AI meeting is July 26-28</li><li>Train Foundation models</li><li>Performance of workflow</li><li>Omniverse</li></ul><p>**Indiana **</p><ul><li>Hire postdoc now that JCS Kadupitiya has graduated and hired by Microsoft</li><li>Soft label paper progressing</li><li>Using Tensorflow for simulation</li></ul><p><strong>Argonne</strong></p><ul><li>Kamil Iskra described publication plan of a paper to ASPLOS and poster to SC</li><li>Baixi noted June 30 abstract deadline and gave the presentation</li><li>1.3 TB dataset</li><li>I/O takes ~81% when run on 8 nodes and 64 GPUs on ThetaGPU</li><li>Clump data and load balance to decrease load time gives a factor of 2.16 speedup</li><li>Use Memory not SSD for storage</li><li>Gregor suggested compressing data in shared memory</li><li>Global arrays and RDMA</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-632e11d8ab45f2682589a545cce0fb97>8.20 - Meeting Notes 05-23-2022</h1><div class=lead>Meeting Notes from 05-23-2022</div><p><strong>Minutes of SBI-FAIR May 23, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Deborah Penchoff, Vikram Jadhao, Shantenu Jha, Geoffrey Fox, Xiaodong Yu, Piotr Luszczek, Baixi Sun, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><ul><li>Geoffrey described substantial progress with Science working group of MLCommons which should have reached first base on June 1 at an ISC BOF</li><li>The diffusion equation surrogate work with Javier Toledo and James Glazier is being written up.</li><li>He also commented on Argonne shuffling performance and use of Big Data collective shuffle primitives that work on disk and memory.</li></ul><p><strong>Tennessee</strong></p><ul><li>Cade Brown is on internship with NVIDIA</li><li>Piotr gave the presentation describes the nice progress with SABATH system introduced by Cade last month.</li><li>SABATH is now available with two applications<ul><li>Keras MNIST</li><li>Cloudmask-0 extended from work of UK group of Tony Hey</li></ul></li><li>SABATH would cache data locally</li><li>Tensorboard visualization support was described</li><li>Add PyTorchsupport to current Tensorflow plus new applications. \</li></ul><p><strong>Rutgers</strong></p><ul><li>Meeting with the Indiana group (Vikram) on adaptive training</li></ul><p>**Indiana **</p><ul><li>Working with Rutgers to agree with last bullet!</li><li>Devising strategy to minimize needed training size</li><li>JCS Kadupitiya in Vikram’s group got his Ph.D. and the Luiddy outstanding research award. He is off to work for Microsoft.</li></ul><p><strong>Argonne</strong></p><ul><li>Baixi gave the Argonne presentation after introduction by Xiaodong</li><li>They are debating between HDF5 or Binary storage</li><li>Changing the I/O middleware to be based on parallel HDF5</li><li>Test done on 16 GPUs corresponding to 2 nodes</li><li>Execution time doesn’t depend much on Batch size. Geoffrey suggested that indicates GPUs not fully utilized so smaller computation does not exploit all internal GPU parallelism</li><li>Baixi reviewed the problems with shuffle being needed every epoch and the challenge when data size large and will not fit in memory and needs disk (small datasets fit into memory)</li><li>The Lustre file system used is bad for small randomly accessed files; typically each image is one file</li><li>The load is manly read with some writes</li><li>The shufflings are all precalculated and the redistribution needed (MPI AllScatter/gather) can be represented as a graph which is imbalanced</li><li>Computation and Data movement are traded off with heuristic solution near to the true minimum</li><li>Parallel HDF5 (using MPI-IO) supports multiple MPI processes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d5cc2405b43f81f520ab2bfe99534d50>8.21 - Meeting Notes 04-25-2022</h1><div class=lead>Meeting Notes from 04-25-2022</div><p><strong>Minutes of SBI-FAIR April 25, 2022, Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Deborah Penchoff, Vikram Jadhao, Shantenu Jha, Geoffrey Fox, Xiaodong Yu, Piotr Luszczek, Cade Brown, Baixi Sun, Jack Dongarra</p><p><strong>Updates</strong></p><p><strong>Virginia</strong></p><ul><li>Discussed continued work on diffusion surrogate with Glazier and Javier Toledo (Edmonton)</li><li>Discussed Fusion surrogate benchmark from Lawrence Livermore</li></ul><p><strong>Tennessee</strong></p><ul><li>Cade Brown presented an update</li><li>Discussed Sentinel 3 benchmark based on UK Cloudmask from MLCommons</li><li>Then discussed FAIR Benchmark platform SLIP which is has been extended to become SABATH</li><li>Described report structure<ul><li>Model format - how universal is this</li></ul></li><li>Has done UK cloudmask and looked at TEvol (2 MLCommons benchmarks)</li><li>Deal with Jupyter notebooks with nbconvert</li><li>Add callbacks to model.fit</li><li>How to do FAIR</li><li>Use Json</li><li>Relation to SciML-Bench <a href=https://github.com/stfc-sciml/sciml-bench>GitHub - stfc-sciml/sciml-bench: SciML Benchmarking Suite for AI for Science</a> and MLCube from MLCommons</li></ul><p><strong>Rutgers</strong></p><ul><li>The IPDPS paper was accepted. This isn’t the final version, but the only publicly available version currently is <a href=https://arxiv.org/abs/2104.04797>[2104.04797] Coupling streaming AI and HPC ensembles to achieve 100-1000x faster biomolecular simulations</a></li><li>Discussed Adversarial autoencoders and use of Alphafold which is expected to do better</li><li>Summit difficult due to IBM containers</li><li>Noted continued study of “2 billion” paper (renamed “Building high accuracy emulators for scientific simulations with deep neural architecture search” <a href=https://arxiv.org/pdf/2001.08055.pdf>https://arxiv.org/pdf/2001.08055.pdf</a>)</li><li>Survey paper</li><li>Noted Proxima by Ian Foster <a href=https://dl.acm.org/doi/abs/10.1145/3447818.3460370>Proxima | Proceedings of the ACM International Conference on Supercomputing</a></li></ul><p>**Indiana **</p><ul><li>Working on scaling recurrent neural net surrogate <a href=https://doi.org/10.1088/2632-2153/ac5f60>https://doi.org/10.1088/2632-2153/ac5f60</a> to more particles</li><li>Ph.D. student JCS Jcs Kadupitiya will defend thesis.</li></ul><p><strong>Argonne</strong></p><ul><li>Baixi presentation</li><li>Described distributed training shuffling problem as a graph</li><li>Cost of training has large data loading time</li><li>Studied increasing standard deviation/mean by redistribution over nodes</li><li>Address Imbalance data loading by moving computetasks to other nodes</li><li>Note large compute variance over GPUs even if batch size fixed, which seems surprising &ndash; why are some GPUs slow?</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ad0df7b5a7a4ca4d394fa6fff6de9da>8.22 - Meeting Notes 03-19-2022</h1><div class=lead>Meeting Notes from 03-19-2022</div><h2 id=minutes-of-sbi-fair-march-19-2022-meeting>Minutes of SBI-FAIR March 19, 2022, Meeting<a class=td-heading-self-link href=#minutes-of-sbi-fair-march-19-2022-meeting aria-label="Heading self-link"></a></h2><ul><li>Present: Kamil Iskra, Vikram Jadhao, Shantenu Jha, Geoffrey Fox, Xiaodong Yu, Piotr Luszczek, Cade Brown, Baixi Sun, Gregor von Laszewski</li></ul><p><strong>Updates</strong></p><p><strong>Rutgers</strong></p><p>A postdoc left unexpectedly and so the surrogate classification work was delayed. The integration of Rutgers software into Vikram’s work is proceeding and will be tested with a Summit allocation.</p><p><strong>Indiana</strong></p><p>Vikram discussed a surrogate paper accepted by Machine Learning: Science and Technology journal <a href=https://doi.org/10.1088/2632-2153/ac5f60>https://doi.org/10.1088/2632-2153/ac5f60</a>. This evolves a modest collection of particles in for example the Lennard-Jones potential obtaining good results with time steps 4000 times that of classic solvers. He also presented at multiple APS sessions. He noted other work using Tensorflow to perform simulations &ndash; a collaboration with another Indiana Engineering faculty.</p><p><strong>Virginia</strong></p><p>Gregor presented on the status of the MLCommons benchmark stressing the difficulties in reconciling GitHub and Jupyter notebooks. Geoffrey noted that these were not quite what you wanted as a scientific electronic notebook as they didn’t support sharing of modified versions and the management of multiple Jupyter notebooks. For example, this project produced 450 notebooks and it is not even easy to search as traditional Google search fails on notebooks.</p><p>Gregor also discussed timing tools</p><p><strong>Tennessee</strong></p><p>Piotr described progress in integrating MLCommons ontologies into the FAIR metadata system. He also noted problems in defining how to run SciML benchmarks with Horovod. Tennessee also submitted a challenge to the Smoky Mountain conference based on Satellite images generalizing the SciML CloudMask benchmark</p><p><strong>Argonne National Laboratory</strong></p><p>Xiaodong introduced the Argonne study of shared I/O. The need for global shuffling at each epoch is potentially an I/O problem but their approach gave almost a factor of 10 improvement (11.4 seconds reduced to 1. seconds).</p><p>Baixi gave a detailed discussion with his usual excellent presentation.</p><p>Geoffrey and Gregor noted the practical challenge of I/O in University shared file systems with both the Earthquake code and an examination of a regular MLPerf benchmark where cloud I/O was much faster than the academic shared file system. The latter problem can be addressed by copying to local disks. Execution from those is a little faster than the cloud numbers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-852e8f9f130ef25f4f6ff038135fb60d>8.23 - Meeting Notes 02-14-2022</h1><div class=lead>Meeting Notes from 02-14-2022</div><p><strong>Minutes of SBI-FAIR February 14 2022 Meeting</strong></p><ul><li>Present: Kamil Iskra, Vikram Jadhao, Geoffrey Fox, Deborah Penchoff, Xiaodong Yu, Piotr Luszczek, Cade Brown, Baixi Sun, Gregor von Laszewski</li></ul><p><strong>Updates</strong></p><p><strong>Tennessee</strong></p><p>A new team member Cade Brown gave a fascinating talk <a href="https://docs.google.com/presentation/d/1sl5MAYodvQyfw46rYG0PHNL5nsf5NQYgrvGlF1Z_kHo/edit?usp=drivesdk">CadeBrown-notes-SBI_Schema</a>. Cade Brown is a new ICL student tasked with designing a schema and tooling for installing, running, and benchmarking ML models. He showed examples using MLCommons Science benchmarks CloudMask and STEMDL. There will be a public website from which you can search models, datasets, and results and run examples. He discussed use of JSON rather than XML and the use of Google’s Firebase JSON database tool. There was a discussion of the sustainability of Firebase (as you need to pay) and the use of containers.</p><p>Geoffrey noted synergy with MLCommons Science Data working group <a href=https://mlcommons.org/en/groups/research-science/>Science Working Group | MLCommons</a>, the Research Data Alliance and Christine KIrkpatrick</p><p><strong>Argonne National Laboratory</strong></p><p>Argonne described the continued work on understanding the performance of distributed training already discussed in the last four meetings. Today&rsquo;s discussion focussed on I/O and included a talk by Baixi which as always was very informative. I/O is a major bottleneck alleviated by caching in either SSD and/or CPU memory. There is a plan for a Parallel I/O and hdf5 paper at SC22. The Hoefler paper at SC21 <a href=https://dl.acm.org/doi/10.1145/3458817.3476181>Clairvoyant prefetching for distributed machine learning I/O | Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</a> has a simulator that ANL used in this analysis. Shuffling is major difficulty as requires access to all the data. There is a fast local version but it is not as good an algorithm as the usual global shuffle. Currently, dataset is 22 GB but it can increase. \</p><p><strong>Indiana</strong></p><p>Vikram reported that his surrogate was ready to deploy and that he has received a Summit allocation to support its training. He had met with Shantenu. He sent Cade Brown a couple of links to a repository that hosts their ML surrogate model and the simulation code used to generate datasets to train and test this model. Hopefully, this surrogate can serve as a test model for the system he is building.</p><p><a href=https://github.com/softmaterialslab/nanoconfinement-md/tree/master/python>https://github.com/softmaterialslab/nanoconfinement-md/tree/master/python</a></p><p><a href=https://github.com/softmaterialslab/nanoconfinement-md/>https://github.com/softmaterialslab/nanoconfinement-md/</a></p><p>You can see the surrogate in action, by launching the tool:</p><p><a href=https://nanohub.org/tools/nanoconfinement/>https://nanohub.org/tools/nanoconfinement/</a></p><p><strong>Virginia</strong></p><p>Progress continues with surrogate for discussion solver. We are writing a second paper on this. Gregor discussed progress with compression.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d4a72ac924bdb915d2e0829bdd4b38d>8.24 - Meeting Notes 01-10-2022</h1><div class=lead>Meeting Notes from 01-10-2022</div><p><strong>Minutes of SBI-FAIR January 10 2022 Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Vikram Jadhao, Geoffrey Fox, Deborah Penchoff, Xiaodong Yu, Jack Dongarra, Shantenu Jha, Piotr Luszczek, Baixi Sun, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Tennessee</strong></p><p>Piotr reported UTK’s continued progress with the FAIR technology in his presentation with a discussion of the ontology needed for SciML and extensions to MLCommons. The choice of YAML versus XML and <a href=https://en.wikipedia.org/wiki/TOML>TOML</a> was discussed. There was a discussion between Piotr and Gregor about that indicated that the YAML format is not sufficient to encode the surrogate and the hardware used for it. An alternative was discussed where one encodes endpoints in the YAML and these endpoints have the detailed metadata/Schema. This is natural in examples that use PyTorch or Tensorflow which could have customized sub-ontologies. Gregor suggested circulating an example to identify if YAML would be nevertheless good enough. The performance surrogate is running on Summit.</p><p><strong>Argonne</strong></p><p>Argonne described the continued work on understanding the performance of distributed training already discussed in the last three meetings with the 2 models, Horovod and the Mirrored Strategy, for ptychoNN surrogate. Baixi new slides are at They are using the latest model from PtychoNN team and testing on the large diffraction and real space data on the 2 distributed training models. Horovod did better on 4, 8 GPU’s; Mirrored on 1,2 GPU’s. They implemented Pytorch DDP to profile and analysis the performance.</p><p><strong>Rutgers</strong></p><ul><li>This continued discussion from last time on work with Vikram on software</li><li>.Progress on Quantum computing surrogate with Ian Foster</li><li>Shantenu also updated work on categorizing surrogates. \</li></ul><p><strong>Indiana</strong></p><p>Vikram reported an update on the time series molecular dynamics surrogate although not using the soft (adding in simulation errors) optimization.</p><p><strong>Virginia</strong></p><p>Geoffrey was distracted by the poor performance of his home internet (now corrected) and did not report solid progress on his diffusion equation solver</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ef2e55e0086e76a1ad6d95adc4dc9c05>8.25 - Meeting Notes 10-21-2021</h1><div class=lead>Meeting Notes from 10-21-2021</div><p><strong>Minutes of SBI-FAIR October 25 2021 Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Vikram Jadhao, Geoffrey Fox, Deborah Penchoff, Xiaodong Yu, Jack Dongarra, Shantenu Jha, Piotr Luszczek, Baixi Sun, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Tennessee</strong></p><p>Piotr reported that paper submitted to IPDPS; and metadata (FAIR) work is continuing</p><p><strong>Virginia</strong></p><p>Geoffrey has summarized 4 possible MLCommons Science Datasets that could be useful for FAIR studies. See recent Argonne preprint</p><p><strong>Indiana</strong></p><p>Vikram Jadhao described his new surrogate paper <a href=https://arxiv.org/abs/2110.14714>[2110.14714] Designing Machine Learning Surrogates using Outputs of Molecular Dynamics Simulations as Soft Labels</a> and quoting from abstract “Here, we show that statistical uncertainties associated with the outputs of molecular dynamics simulations can be utilized to train artificial neural networks and design machine learning surrogates with higher accuracy and generalizability. We design soft labels for the simulation outputs by incorporating the uncertainties in the estimated average output quantities and introduce a modified loss function that leverages these soft labels during training to significantly reduce the surrogate prediction error for input systems in the unseen test data. The approach is illustrated with the design of a surrogate for molecular dynamics simulations of confined electrolytes to predict the complex relationship between the input electrolyte attributes and the output ionic structure. The surrogate predictions for the ionic density profiles show excellent agreement with the ground truth results produced using molecular dynamics simulations.”</p><p><strong>Rutgers</strong></p><ul><li>Collaboration with Vikram has started</li><li>Classification of surrogates introduced 6 classes and analyzed many new papers</li><li>Gordon Bell submission involved Caltech + DOE Labs + San Diego and used surrogates at multiple levels &ndash; it studied how to balance effort between them. The application concerned Delta Covid.</li></ul><p><strong>Argonne</strong></p><p>Kamil and Xiaodong described the continued work on understanding the performance of distributed training already introduced last month. Baixi gave the presentation . Next month will see a new dataset and new results.</p><p>Hyperparameters were tuned for ptychoNN surrogate on Horovod and the Mirrored Strategy.</p><p>The current approach is synchronous but will look at asynchronous methods.</p><p>We agreed on the next meeting date November 29.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d0723c0cf7daa1c408381ee87d3ab00>8.26 - Meeting Notes 09-27-2021</h1><div class=lead>Meeting Notes from 09-27-2021</div><p><strong>Minutes of SBI-FAIR September 27 2021 Meeting</strong></p><p><strong>Present:</strong> Kamil Iskra, Vikram Jadhao, Geoffrey Fox, Deborah Penchoff, Xiaodong Yu, Jack Dongarra, Shantenu Jha, Piotr Luszczek, Pete Beckman, Baixi Sun, Gregor von Laszewski</p><p><strong>Updates</strong></p><p><strong>Indiana/Virginia</strong></p><p>Vikram has a new surrogate and is finalizing a paper on it. He will talk to Shantenu soon.</p><p><strong>Rutgers</strong></p><p>Shantenu affected by hurricane</p><ol><li>Developing 3 layer simulations with surrogate at each level</li><li>ML driven HPC motifs/patterns identified in research to be reported at November meeting<ol><li>DeepDriveMD ensemble is one example</li><li>climate science simulations gives surrogates that select best simulation</li><li>Link with observation link seen in climate, materials and biomolecular science</li></ol></li></ol><p><strong>University of Tennessee</strong></p><ol><li>Workshop in April 4-7 2022 at UTK</li><li>Performance surrogate paper to IPDPS; excellent speedup but not 2 billion</li><li>FAIR ontologies will resume after this paper</li></ol><p><strong>Argonne</strong></p><ol><li>Yu introduced their GPU scheduling work and investigation of the surrogate model training change scalability</li><li>Baixi Sun gave a detailed presentation on Distributed Training On PtychoNN<ol><li>Utilized the Horovod framework on ptychoNN model.</li><li>Tested the Horovod performance for different number of GPUs on single node and multiple nodes using Ring All-Reduce</li><li>Tried Mirrored Strategy framework on ptychoNN model.</li><li>Tested the performance for different number of GPUs on single node.</li><li>Debugging of the Mirrored Strategy framework for distributed training.</li><li>Presented performance numbers with MNIST and ptychoNN</li><li>Updated our versions of code on our gitlab repository and wiki documentation.</li></ol></li><li>Links for more details are:
8. This is the official documentation for Horovod: <a href=https://horovod.readthedocs.io/en/stable/keras.html>Horovod with Keras — Horovod documentation</a> .
9. And this is the thetaGPU Horovod tutorial: <a href=https://www.alcf.anl.gov/support-center/theta-gpu-nodes/distributed-training-thetagpu-using-data-parallelism>Distributed training on ThetaGPU using data parallelism | Argonne Leadership Computing Facility</a> .
10. This is the official documentation for Mirrored Strategy: <a href=https://keras.io/guides/distributed_training/>Multi-GPU and distributed training</a> (Section &ldquo;Single-host, multi-device synchronous training&rdquo;).
11. To be specific, the code I ran on thetaGPU is currently in our <strong>private</strong> Gitlab repository: <a href=https://gitlab.com/SBI-HPC/benchmark_suite/-/tree/main/ptychography>https://gitlab.com/SBI-HPC/benchmark_suite/-/tree/main/ptychography</a> . (Please note that for Mirrored Strategy I am currently debugging on it so the latest stable version of code has not committed yet, will come soon!).
12. The guidance of using those code on thetaGPU is written in the Gitlab wiki: <a href=https://gitlab.com/SBI-HPC/benchmark_suite/-/wikis/PtychoNN-Distributed-Training-on-ThetaGPU>https://gitlab.com/SBI-HPC/benchmark_suite/-/wikis/PtychoNN-Distributed-Training-on-ThetaGPU</a>.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-61514199af825bada68cfa586f122586>8.27 - Meeting Notes 08-30-2021</h1><div class=lead>Meeting Notes from 08-30-2021</div><p><strong>Minutes of Meeting August 30, 2021</strong></p><p><strong>Present:</strong> Kamil Iskra, Vikram Jadhao, Geoffrey Fox, Deborah Penchoff, Xiaodong Yu, Jack Dongarra, Shantenu Jha, Piotr Luszczek, Pete Beckman, Baixi Sun</p><p><strong>Updates</strong></p><ul><li><strong>Rutgers:</strong> Progress with recruiting problems. Highlighted a new paper <a href=https://doi.org/10.1021/acs.jcim.8b00839>https://doi.org/10.1021/acs.jcim.8b00839</a> on molecular benchmarks from Benevolent AI <a href=https://www.benevolent.com/guacamol>GuacaMol: Benchmarking Models for De Novo Molecular Design</a>. Peter Coveney Company in London</li><li><strong>Tennessee</strong> continues work on the performance surrogate model. Tune hyperparameters. Build from small runs. Report in October. Works on simulations or data analytics. Unlike ATLAS aimed at problems with runs that take a large time</li><li>**Argonne. **Pete noted by email a new paper <a href=https://arxiv.org/pdf/2104.12871.pdf>Why AI is Harder Than We Think</a> with a cautionary tale.<ul><li>Baixi Sun from Washington State University was introduced as a new student on project</li><li>Xiaodong discussed their 3 use cases. Convert notebooks to python scripts and run in multinode fashion</li><li>Using ALCF the first usage mode is based on Jupyter notebooks and second usage mode is batch</li><li>ALCF likes Jupyter notebooks. Also note <a href=https://docs.olcf.ornl.gov/services_and_applications/jupyter/overview.html>Jupyter notebooks at ORNL</a></li></ul></li><li><strong>Indiana/Virginia</strong>. Vikram Jadhao presented on surrogates for soft materials<ul><li>This reviewed highlights from the field and then focussed on his work</li><li>Word surrogate not often used in field</li><li>The review covered SorbNet from MInnesota, ab initio simulation from Toronto and pair correlation function of liquids from UIUC group Aluru</li><li>Vikram’s application was confined electrolytes where surrogate relates structure to attributes</li><li>Good use in education using nanoHUB deployment</li><li>Nice performance slide</li><li>EXTENDED Predictions were not as good as original ones</li><li>Need to quantify and improve accuracy &ndash; how? Average over all quantities but worse near the wall. COULD weight those points more in loss<ul><li>Common in surrogates, that error is dominated by “special” regions &ndash; boundaries, singularities etc. as work of Geoffrey with James Glazier on diffusion equation for cell modelling.</li></ul></li><li>Look at reducing needed training size</li><li>Will evaluate using Rutgers software infrastructure</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a2f2a49a9c251a60b8b3d8d394c02960>8.28 - Meeting Notes 07-26-2021</h1><div class=lead>Meeting Notes from 07-26-2021</div><p><strong>Minutes of Meeting July 26, 2021</strong></p><p>Shantenu led a discussion of surrogates noting his work was delayed by a loss of a postdoc. Shantenu divided Surrogates into 3 areas</p><ul><li>**MLinHPC **as in Climate in Oxford paper <a href=https://arxiv.org/abs/2001.08055>[2001.08055] Building high accuracy emulators for scientific simulations with deep neural architecture search</a> giving speedups over a billion but there are some curious features of this work. Directly replace full simulation but also can calculate potentials as in DeepMD <a href=https://arxiv.org/pdf/2005.00223.pdf>https://arxiv.org/pdf/2005.00223.pdf</a> where 80-90% of computational cost is potential computation.<ul><li>Docking with Austin Clyde and Argonne group (including Shantenu and Rick) <a href=https://arxiv.org/abs/2106.07036>[2106.07036] Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening</a><ul><li>Factor of 10 no loss of accuracy</li><li>NVIDIA helped on performance</li></ul></li></ul></li><li><strong>MLaboutHPC</strong> Here ML guides the simulation such as in choosing ensemble and his DeepDriveMD Westpahl algorithm shows a Factor of 100 compared to Anton<ul><li>Shantenu used a VAE but list of 7 methods on slide 8</li></ul></li><li><strong>MLoutHPC</strong> where Shantenu gave one example where one optimizes campaign across scales using Reinforcement learning with Austin Clyde model at top</li></ul><p>Shantenu presented PY2 and PY3 plans</p><p>In PY2 primary goals are:</p><ul><li>(mini-)Review of surrogates in HPC &ndash; Volunteers? See later</li><li>Formalizing Performance measures (MLinHPC)<ul><li>Three scenarios discussed above: Climate, Docking, Potentials</li></ul></li><li>Experimenting with Performance (MLoutHPC)<ul><li>Use DeepDriveMD to support different surrogates (Table 1) for common physical model (system)</li></ul></li></ul><p>In PY3</p><ul><li>tackle (more) complex problem of MLoutHPC</li></ul><p>AlphaFold2 (Google) and RoseTTaFold (Baker at Washington) <a href=https://www.nature.com/articles/d41586-021-01968-y>DeepMind&rsquo;s AI for protein structure is coming to the masses news</a> BOTH released</p><p>CASP said protein folding solved from AlphaFold2 but RosettaFold is cheaper and as good as AlphaFold2. This could be an opportunity</p><p>Beckman noted we see a science transformation using FAIR Methodology.</p><p>Rick Stevens has challenged “How much did Go AI cost”</p><p>Dataset size is a serious issue.</p><ul><li><a href=https://github.com/deepmind/alphafold>deepmind/alphafold: Open source code for AlphaFold.</a> notes The total download size for the full databases is around 415 GB and the total size when unzipped is 2.2 TB. Please make sure you have a large enough hard drive space, bandwidth and time to download. We recommend using an SSD for better genetic search performance.</li><li>Hurricane simulation will become inference</li><li>Doe strategy train leave data where it is similar to medical federated learning</li><li>Vikram noted that material science led to smaller datasets as just output final results and not the full trajectory</li></ul><p>We discussed having a session at The Argonne Training Program on Extreme-Scale Computing (ATPESC) in 2022</p><p>Next month we will consider Implications for the project. Vikram and Shantenu volunteered</p></div><div class=td-content style=page-break-before:always><h1 id=pg-424ae80d730642b73c7dc54af84bdebd>8.29 - Meeting Notes 06-29-2021</h1><div class=lead>Meeting Notes from 06-29-2021</div><p><strong>Minutes of Meeting June 29, 2021</strong></p><p><strong>Annual Report</strong></p><p>This meeting focussed on getting the final version of the DOE annual report which was submitted the following day by each institution.</p><p><strong>Next Meeting</strong></p><p>Our meetings are 1 pm Eastern on the 4th Monday of each month</p><p>This implies Monday, July 25, 1 pm at zoom <a href=https://iu.zoom.us/j/2301429329>https://iu.zoom.us/j/2301429329</a></p><p>In the July meeting, Shantenu Jha will lead a discussion of surrogates, postponed from June</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cd876d9c980694d77f84f5268583d645>8.30 - Meeting Notes 05-24-2021</h1><div class=lead>Meeting Notes from 05-24-2021</div><p><strong>Minutes of Meeting May 24, 2021</strong></p><p><strong>Links for Today’s Meeting</strong></p><p>Powerpoint of Argonne Talk <a href="https://drive.google.com/file/d/1VOR2V0c86adXgzoNqCwQe4O7di0qbBP5/view?usp=sharing">2021-05-SBI-ANL.pptx</a></p><p>PDF of Argonne Talk <a href="https://drive.google.com/file/d/1UlQ1YGS6J72vcnz65I_I54K9fyWLMiXd/view?usp=sharing">2021-05-SBI-ANL.pdf</a></p><p><strong>Present</strong></p><p><strong>Argonne:</strong> Min Si, Xiaodong Yu</p><p>**Indiana: **Geoffrey Fox, Vikram Jadhao, Gregor von Laszewski</p><p><strong>Rutgers:</strong> Shantenu Jha</p><p><strong>UTK:</strong> Jack Dongarra, Piotr Luszczek</p><p><strong>Argonne Presentation</strong></p><p>Xiaodong Yu’s described 3 surrogates being developed at Argonne</p><p><strong>Application 1</strong> **PtychoNN: Ptychographic Imaging Reconstruction phase reconstruction **</p><p>Here the challenge is to determine phases from Xray scattering data with <a href=https://aip.scitation.org/doi/10.1063/5.0013065>paper</a>. The surrogate is being extended to run using Horovod on the multi-GPU ThetaGPU system.</p><p><strong>Application 2: Geophysical Forecasting</strong></p><p>This involves LSTM forecast models combined with a neural architecture search NAS using deephyper in original<a href=https://dl.acm.org/doi/abs/10.5555/3433701.3433711> paper</a> which ran on Theta without GPUs.</p><p><strong>Application 3:</strong> <strong>Molecular dynamics (MD) simulation</strong></p><p>This is multiscale modeling of SARS-CoV-2 in the CANDLE project which received the 2020 ACM Gordon Bell Special Prize for High Performance Computing-Based COVID-19 Research.</p><p>Shantenu Jha was a co-author on their <a href=https://www.biorxiv.org/content/10.1101/2020.11.19.390187v1>paper</a> “AI-Driven Multiscale Simulations Illuminate Mechanisms of SARS-CoV-2 Spike Dynamics”.</p><p><strong>Other Business</strong> We discussed adding material to the <strong>website</strong>.</p><p><strong>Annual Report</strong></p><p>We just received the request from DOE for an annual report abstracted below, We could discuss (unfortunately it is due before our next meeting) a common text that we could use as part of each report.</p><p>The Office of Advanced Scientific Computing Research (ASCR) within the Department of Energy Office of Science requests that you submit a Progress Report for the award listed below. To create and submit the Progress Report, please use the DOE Office of Science <a href=https://pamspublic.science.energy.gov/webPAMSEPSExternal/Interface/Common/AccessControl/login.aspx>Portfolio Analysis and Management System (PAMS)</a>.</p><p><strong>Task:</strong> Submit Progress Report (<a href="https://pamspublic.science.energy.gov/webPAMSExternal/Interface/Tasks/AwardeeReport/AwardeeReportList.aspx?TaskTypeCode=12">Link</a>)</p><p><strong>Due Date:</strong> 06/24/2021 5:00 PM ET</p><p><strong>Reporting Period:</strong> 09/23/2020 - 09/22/2021</p><p><strong>Next Meeting</strong></p><p>Our meetings are 1 pm Eastern on the 4th Monday of each month</p><p>This implies Monday, June 28, 1 pm at zoom <a href=https://iu.zoom.us/j/2301429329>https://iu.zoom.us/j/2301429329</a></p><p>In the June meeting, Shantenu Jha will lead a discussion of surrogates.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-593c8019dfc9c8121a0bba92a829b7d2>8.31 - Meeting Notes 04-19-2021</h1><div class=lead>Meeting Notes from 04-19-2021</div><p><strong>Minutes of Meeting April 19, 2021</strong></p><p><strong>Links for Today’s Meeting</strong></p><ul><li>Indiana Update plus Overall Project Remarks <a href="https://docs.google.com/presentation/d/1-Td2Fz9yk3GNwa0Z2jbsQk8I_02bMMUexLTRCO09bt4/edit?usp=sharing">SBI-Meeting-IU-April19-2021</a></li><li>Tennessee Update Presentation <a href="https://drive.google.com/file/d/1mv_EpqETFrMmt4fxYSYLrgDOzOL1xlqU/view?usp=sharing">sbi20210419.pdf</a></li></ul><p><strong>Updates</strong></p><ul><li><strong>Argonne</strong> postponed their update to the next meeting and the other 3 sites gave updates.</li><li><strong>Indiana</strong> discussed SciMLBench from the UK with its first <a href=https://github.com/stfc-sciml/sciml-bench>release</a> and the related MLCommons Science benchmarking. With surrogates, Jadhao will work on the nanoengineering one in the Fall and Fox completed an initial study of a virtual tissue surrogate <a href=https://arxiv.org/abs/2102.05527>[2102.05527] Deep learning approaches to surrogates for solving the diffusion equation for mechanistic real-world simulations</a>.</li><li><strong>Tennessee</strong> gave a comprehensive report covering their Surrogate Performance Model for Autotuning; their <a href=https://code.ornl.gov/FK6D/FK6D>FK6D / ASGarD · GitLab</a> project aimed at a later release of SCiMLBench and an insightful analysis of issues and needed ontologies for a FAIR approach to benchmark data. The discussion pointed out that FAIR does not address areas like validation, verification, and reproducibility. Piotr introduced broad categories: Hardware, firmware, dataset, software, measurements. We know from MLPerf that I/O specification and measurement are nontrivial. The mode of execution: capability or capacity(high-throughput) needs to be specified. Gregor noted complications from the use of containers that can hide software versioning. Christine Kirkpatrick&rsquo;s <a href="https://drive.google.com/file/d/1VECDSbrh8N6mvn7g4bsBkSGesiOyAtAW/view?usp=sharing">Advancing AI through MLCommons</a> to MLCommons Benchmark-Infra WG April 6 highlighted tension between the flexibility of free text and FAIR machine readability</li><li>**Rutgers **Shantenu Jha discussed recent work by his group on computational performance. He pointed out a recent paper by Alexandru Iosup on <a href=https://dl.acm.org/doi/pdf/10.1145/3447545.3451185>GradeML: Towards Holistic Performance Analysis for Machine Learning Workflows</a></li></ul><p><strong>Discussion and Action Items</strong></p><ul><li>We agreed to start two working groups on FAIR (coordinated by Piotr) and Surrogates (coordinated by Shantenu). The scope of both groups was unclear as yet and should be discussed in meetings</li><li>There was a discussion of access to computers across the collaboration</li><li>We discussed Surrogate Software and Benchmark software with work of Deep500 (Torsten Hoefler of ETH Zurich), GradeML, MLCube, SciMLBench mentioned. We need to relate it to FAIR</li><li>We still need to implement SBI repository</li><li>We agreed in the March meeting to enhance the website with updated (after proposal) information. Please send your GitHub ID’s to Gregor <a href=mailto:laszewski@gmail.com>laszewski@gmail.com</a> so he can enable to directly edit web site<ul><li>Only Gregor contributed so far with core site <a href=https://sbi-fair.github.io/>https://sbi-fair.github.io/</a></li><li>Not all GitHub invitations have been accepted</li></ul></li><li>Deborah Penchoff of UTK identified a t<a href="https://docs.google.com/document/d/15wu_5Z76lmOAyNchwZynhudparJ8qKnsvCmkTq9jYec/edit?usp=sharing">emplate</a> for DOE annual report. We should accumulate the needed contributions</li><li>We agreed to set the next meeting for 1-2 pm Eastern May 24 2021 at the usual zoom <a href=https://iu.zoom.us/j/2301429329>https://iu.zoom.us/j/2301429329</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f6b9a39a5bbf5d8e177dea442a2437a6>8.32 - Meeting Notes 03-23-2021</h1><div class=lead>Meeting Notes from 03-23-2021</div><p><strong>Minutes of Meeting March 23 2021</strong></p><p><strong>Links for Today’s Meeting</strong></p><ul><li>Argonne Update Presentation <a href="https://docs.google.com/presentation/d/11gQpwLgifWoV0U9YeBroq3c_7ekOx1qG0JV08h4GOEM/edit?usp=sharing">SBI- ANL-202103-Updates</a></li><li>Argonne Surrogate Application <a href="https://drive.google.com/file/d/1TM3HSHRPJAMnSstqRxNSEnjjQzi_x8n2/view?usp=sharing">SBI-ANL-202103-Ptycho-Surrogate.pptx</a></li><li>Tennessee Update Presentation <a href="https://docs.google.com/presentation/d/1NBbMLTlNbIJ6jXaFPyIbTfIMqO5YPMFgU0HQaT_taRc/edit?usp=sharing">SBI @ UTK 2k21</a></li><li>Indiana University Update Presentation <a href="https://docs.google.com/presentation/d/1-nFeWngmbb1Gxi7BztBwzEZzib_hJ4wzwiM1j6KfIWU/edit?usp=sharing">SBI-Meeting-IU-Mar23-2021</a></li><li>Christine Kirkpatrick’s MLCommons Science working group talk on FAIR Metadata <a href="https://drive.google.com/file/d/15iQTE99xAUIPNe9bIY_l8Xsv2TVLyqoR/view?usp=sharing">mlcommons_fair_0221.pptx</a></li><li>Compilation of MLCommons logged metadata <a href="https://drive.google.com/drive/folders/17RH-wAlEpiU5dcgKB-7kg5Z_zQzBOpfR?usp=sharing">Logging Info MLCommons</a></li></ul><p>The 4 sites all gave updates with presentations listed above.</p><p><strong>Indiana</strong> largely discussed work with MLCommons Science research working group</p><ul><li>Benchmark collection which will eventually include surrogates</li><li>Benchmark Technology and FAIR metadata</li></ul><p><strong>Argonne</strong> presented substantial progress with</p><ul><li>The hiring of a new postdoc Xiaodong Yu with substantial experience</li><li>Identification of several surrogates including those that don’t work e.g. give insufficient accuracy</li><li>Use of ThetaGPU</li></ul><p>**Tennessee **reported substantial progress with</p><ul><li>Examination of MLFlow and its metadata which support many storage formats but are missing FAIR features</li><li>ONNX Open Neural Network Exchange which currently has no science or surrogate examples</li><li>The N to N issues of matching many inputs to many outputs’</li><li>Performance surrogate model for Autotuning work in progress</li></ul><p><strong>Rutgers</strong> (no presentation) discussed two activities</p><ul><li>Effective performance where a new student will join.</li><li>Surrogates corresponding to two Gordon Bell prize winners at SC20 extending from Rutgers work with Argonne (autoencoders for collective coordinates to move through phase space quickly) to the other winner from Princeton where AI learned the complex potential.</li></ul><p><strong>Action Items</strong></p><ul><li>We agreed to set the next meeting for 1-2 pm Eastern April 19 2021 at the usual zoom <a href=https://iu.zoom.us/j/2301429329>https://iu.zoom.us/j/2301429329</a></li><li>We agreed to enhance the web site with updated (after proposal) information. Please send your GitHub ID’s to Gregor <a href=mailto:laszewski@gmail.com>laszewski@gmail.com</a> so he can enable to directly edit web site</li><li>Shantenu agreed to coordinate a surrogate working group after 4 weeks</li><li>Piotr agreed to coordinate cross-institution FAIR activities including issues of MLCommons <a href="https://drive.google.com/drive/folders/17RH-wAlEpiU5dcgKB-7kg5Z_zQzBOpfR?usp=sharing">metadata</a> and Christine Kirkpatrick’s <a href="https://drive.google.com/file/d/15iQTE99xAUIPNe9bIY_l8Xsv2TVLyqoR/view?usp=sharing">work</a></li><li>Argonne will investigate Yu giving a short presentation</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b80aeca90aa52cd52e412f58032ccd34>8.33 - Meeting Notes 02-20-2021</h1><div class=lead>Meeting Notes from 02-20-2021</div><h2 id=university-of-tennessee-knoxville>University of Tennessee Knoxville<a class=td-heading-self-link href=#university-of-tennessee-knoxville aria-label="Heading self-link"></a></h2><ul><li>Deborah Penchoff joining the team</li><li>UTK Schema</li><li>MLFlow &ndash; reproducibility</li><li>Is training repeatable</li><li>Need to have a group on this</li><li>UTK have their own surrogates science and performance</li><li>Storage</li><li>Uq</li><li>Hardware</li></ul><h2 id=rutgers-university>Rutgers University<a class=td-heading-self-link href=#rutgers-university aria-label="Heading self-link"></a></h2><ul><li>Performance of surrogates</li><li>What does it mean</li><li>Gordon bell prizes</li><li>Deepdrivemd greatly advanced</li><li>Working with Princeton Gordon Bell</li><li>2 billion paper</li></ul><h2 id=argonne-national-laboratory>Argonne National Laboratory<a class=td-heading-self-link href=#argonne-national-laboratory aria-label="Heading self-link"></a></h2><ul><li><p>Clear plans</p></li><li><p>Candle</p></li><li><p>Paper creates a surrogate howto &ndash; GCF forgets this</p></li><li><p>DOE_FAIR2020-Surrogates</p></li></ul><h2 id=github-site-infrastructure>Github site infrastructure<a class=td-heading-self-link href=#github-site-infrastructure aria-label="Heading self-link"></a></h2><ul><li><p>Web site built on Github - Possible Hugo web site</p></li><li><p>Form Google group</p></li><li><p>Form working groups</p></li><li><p>Infrastructure & Benchmarking Tech</p></li><li><p>Metadata/FAIR</p></li><li><p>Surrogates</p></li></ul><p>All meet once a month</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2770df73b2e02cb315a180647d6fd023>8.34 - Meeting Notes 01-20-2021</h1><div class=lead>Meeting Notes from 01-20-2021</div><p>**Indiana University **</p><p>Report <a href="https://docs.google.com/presentation/d/1Hr_5HzeA8wRL5GGVCuok2Rfuqis7VA3I0jn7KhNEY6g/edit?usp=sharing">SBI-Meeting-IU-Jan20-2021</a></p><p><strong>University of Tennessee Knoxville</strong></p><p>Report <a href="https://docs.google.com/presentation/d/1NBbMLTlNbIJ6jXaFPyIbTfIMqO5YPMFgU0HQaT_taRc/edit?usp=sharing">SBI @ UTK 2k21</a></p><p>Deborah Penchoff joining the team</p><p>UTK Schema</p><p>MLFlow &ndash; reproducibility</p><p>Is training repeatable</p><p>Need to have a group on this</p><p>UTK have their own surrogates science and performance</p><p>Storage</p><p>Uq</p><p>Hardware</p><p><strong>Rutgers</strong></p><p>**Report **<a href="https://docs.google.com/presentation/d/1fscM2mVcSSh-D-9DNxPde7r47Y-efKZ-7KcEpcGFic4/edit?usp=sharing">SBI-Rutgers Jan 20-2021</a></p><p>Performance of surrogates</p><p>What does it mean</p><p>Gordon bell prizes</p><p>Deepdrivemd greatly advanced</p><p>Working with Princeton Gordon Bell</p><p>2 billion paper</p><p><strong>Argonne</strong></p><p>Report <a href="https://docs.google.com/presentation/d/1Hr_5HzeA8wRL5GGVCuok2Rfuqis7VA3I0jn7KhNEY6g/edit?usp=sharing">SBI-Meeting-IU-Jan20-2021</a></p><p>Clear plans</p><p>Candle</p><p>Paper creates a surrogate howto &ndash; GCF forgets this</p><p>Github site infrastructure</p><p>Web site built on Github - Possible Hugo web site</p><p>Form Google group</p><p>Form working groups</p><p>Infrastructure & Benchmarking Tech</p><p>Metadata/FAIR</p><p>Surrogates</p><p>All meet once a month</p></div><div class=td-content style=page-break-before:always><h1 id=pg-68f9113b50693620ee9892f038d4139b>9 - Contribution Guidelines</h1><div class=lead>How to contribute to the docs</div><div class="pageinfo pageinfo-primary"><p>More informatio nwill be here soon &mldr;</p></div><p>The Web Site is hosted on Github and can be modified with pull requests.</p><p>To edit the About page, use these links:</p><ul><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/blob/main/content/en/_index.html>Front Page</a></li><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/blob/main/content/en/about/_index.html>About Page</a></li><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/blob/main/content/en/docs/Publications/refs.bib>Publications</a></li><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/tree/main/content/en/blog/news>Blog News</a></li><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/tree/main/content/en/blog/releases>Blog Releases</a></li><li><a href=https://github.com/sbi-fair/sbi-fair.github.io/blob/main/content/en/docs/Contribution%20Guidelines/_index.md>This Page</a></li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="User mailing list" aria-label="User mailing list"><a target=_blank rel=noopener href=https://groups.google.com/g/sbi-fair/c/YuwS0vegIJU/m/aWmNlEU7AAAJ aria-label="User mailing list"><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Gregor von Laszewski" aria-label="Gregor von Laszewski"><a target=_blank rel=noopener href=https://laszewski.github.io aria-label="Gregor von Laszewski"><i class="fab fa-twitter"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title="Web Site GitHub" aria-label="Web Site GitHub"><a target=_blank rel=noopener href=https://github.com/sbi-fair/sbi-fair.github.io aria-label="Web Site GitHub"><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title="Organization GitHub" aria-label="Organization GitHub"><a target=_blank rel=noopener href=https://github.com/sbi-fair aria-label="Organization GitHub"><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2018&ndash;2025
<span class=td-footer__authors>SBI FAIR Authors |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span><span class=ms-2><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></span></div></div></div></footer></div><script src=/js/main.min.21eb2c7b0fd82c0b22fa631416dc800bb198ba4ee5abe2276f555c9cc7863751.js integrity="sha256-Iessew/YLAsi+mMUFtyAC7GYuk7lq+Inb1VcnMeGN1E=" crossorigin=anonymous></script>
<script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script></body></html>